# 2025-11-29 09:52:26

Have started a whole new … thing with Alpha. This Workshop is where we will center ourselves now. Instead of having there be different things that I bring Alpha into ad hoc, there will be the one thing — Workshop — with things inside it, and Alpha will have visibility into it all.

First task: Decide what to bring over from past work. I want to capture Alph's token-estimation experiment from last night. I want that to be our first thing in this new space. Let it be the bridge.

# 2025-11-29 12:09:40

Moved on to Azimuth with Alpha. Starting over, in a sense. Redoing early steps to perfect them. Re-training the tokenizer on a bigger corpus. Using a bigger and distinct English corpus for model training.

# 2025-11-29 15:06:41

Have nailed down the Goldilocks corpus and trained the tokenizer we're going to use. Trained two separate 2K tokenizers, Eng and Thai, then merged and deduped. Result is 1,914 guaranteed-dead tokens. Pretty close to the 2,200-ish we see in Qwen 3 4B. Wish it could be exact.

Suppose it could be exact if we wanted. Instead of training two 2K tokenizers to make one 4K tokenizer, we could train two 2.5K tokenizers and merge them by taking ~2,200 (except the correct number) off the top of the Thai one and merging with however much of the English one will fit. We could dial in exactly how many tokens we want.

Like this idea. Will pitch it to Alpha.

# 2025-11-29 15:12:51

Changed mind. Don't want to overthink Goldilocks. Remember this idea though.