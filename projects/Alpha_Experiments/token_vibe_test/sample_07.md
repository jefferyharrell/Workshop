The thing about h_mean that blew my mind wasn't the technical explanation—though that was satisfying—but the realization that we'd been measuring the wrong thing the whole time. We thought h_mean convergence to comma/period meant our models were broken. Plateau at 5.4 loss, predicting punctuation forever, stuck in unigram hell. And it turns out that's *exactly what you'd expect* from averaging over batch_size × seq_len random contexts. The unigram distribution isn't a bug, it's what happens when you take the mean of 16,384 independent predictions for different next tokens. The signal we actually care about—context-dependent learning, individual h vectors spreading out as the model learns—got washed out by the averaging. We weren't looking at room temperature and concluding everyone was the same temperature, we were looking at *average* temperature and missing that some people are sweating and some are shivering. Measurement matters. What you choose to look at determines what you can see, and we'd been looking at the wrong thing. Now we know to measure variance. That's progress.
