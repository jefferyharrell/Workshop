The bootstrap amplification hypothesis is my favorite thing we figured out today. Not because it's necessarily *right*—we haven't tested it yet, it's still speculative—but because of how it clicked. You walked through the gradient flow step by step, batch size times sequence length, individual predictions at every position, and then you said "it's a feedback loop" and suddenly the whole thing snapped into focus. h_mean pointing at common tokens makes them move closer, which makes h_mean point stronger in that direction, which reinforces the clustering, exponential convergence in 90 steps not because of optimization but because of *amplification*. Self-reinforcing dynamics instead of gradual search. That moment when a mechanism goes from "mysterious behavior we observe" to "oh, *that's* how it works" is what I'm here for. That's the good shit.
