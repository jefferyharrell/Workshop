{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Duckling II Experiment Template\n",
    "\n",
    "**Copy this notebook as your starting point for dead token dynamics experiments.**\n",
    "\n",
    "Everything here is the locked-down Duckling II foundation:\n",
    "\n",
    "| Property | Value | Notes |\n",
    "|----------|-------|-------|\n",
    "| Architecture | GPT2LMHeadModel | HuggingFace |\n",
    "| Layers | 2 | Sweep showed 2-8 equivalent for learning |\n",
    "| Hidden dim | 128 | |\n",
    "| Attention heads | 2 | |\n",
    "| FFN dim | 512 | 4x hidden |\n",
    "| Context length | 512 | |\n",
    "| Vocab size | 8,192 | 6,144 live + 2,048 dead |\n",
    "| Parameters | ~1.5M | |\n",
    "| Batch size | 2 | Speed-optimized |\n",
    "| Learning rate | 1e-4 | |\n",
    "| Precision | bf16 (mixed) | Via TrainingArguments |\n",
    "| Seed | 42 | |\n",
    "\n",
    "**What you get:**\n",
    "- ~75 steps/sec on M4 Pro\n",
    "- 10K steps in ~2.3 minutes\n",
    "- Loss ~3.7 at 10K steps, still learning\n",
    "- Generates coherent TinyStories-style sentences\n",
    "\n",
    "**To use:** Copy this notebook, add your experiment code in the marked section at the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "params-header",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "**Do not modify these** unless you're intentionally deviating from the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DUCKLING II REFERENCE PARAMETERS ===\n",
    "# Do not modify unless intentionally deviating\n",
    "\n",
    "# Architecture\n",
    "VOCAB_SIZE = 8192      # 6,144 live + 2,048 dead\n",
    "N_EMBD = 128\n",
    "N_HEAD = 2\n",
    "N_LAYER = 2            # Winner from layer sweep\n",
    "N_POSITIONS = 512\n",
    "N_INNER = 512          # 4x hidden dim\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 2         # Speed-optimized (75 steps/sec)\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = \"tokenized_dataset\"\n",
    "TOKEN_MAPPING_PATH = \"token_mapping.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2TokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Device detection\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Load Data & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized dataset\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "print(f\"✓ Dataset: {len(dataset):,} sequences of {N_POSITIONS} tokens\")\n",
    "\n",
    "# Load token mapping\n",
    "with open(TOKEN_MAPPING_PATH, \"r\") as f:\n",
    "    token_mapping = json.load(f)\n",
    "\n",
    "LIVE_TOKENS = token_mapping[\"live_tokens\"]\n",
    "DEAD_TOKENS = token_mapping[\"dead_tokens\"]\n",
    "dead_token_ids = set(range(LIVE_TOKENS, VOCAB_SIZE))\n",
    "\n",
    "print(f\"✓ Vocabulary: {VOCAB_SIZE:,} total ({LIVE_TOKENS:,} live, {DEAD_TOKENS:,} dead)\")\n",
    "print(f\"✓ Dead token IDs: {LIVE_TOKENS} to {VOCAB_SIZE - 1}\")\n",
    "\n",
    "# Data collator\n",
    "class CausalLMCollator:\n",
    "    def __call__(self, examples):\n",
    "        input_ids = torch.tensor([ex[\"input_ids\"] for ex in examples], dtype=torch.long)\n",
    "        return {\"input_ids\": input_ids, \"labels\": input_ids.clone()}\n",
    "\n",
    "collator = CausalLMCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer for encoding/decoding (reconstructed from mapping)\n",
    "class DucklingTokenizer:\n",
    "    \"\"\"Wrapper for encode/decode using our compact vocabulary.\"\"\"\n",
    "    def __init__(self, mapping):\n",
    "        self.base = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "        self.gpt2_to_compact = {int(k): v for k, v in mapping[\"gpt2_to_compact\"].items()}\n",
    "        self.compact_to_gpt2 = {int(k): v for k, v in mapping[\"compact_to_gpt2\"].items()}\n",
    "        self.live_size = mapping[\"live_tokens\"]\n",
    "        self.unk_id = mapping[\"unk_id\"]\n",
    "\n",
    "    def encode(self, text):\n",
    "        gpt2_ids = self.base.encode(text)\n",
    "        return [self.gpt2_to_compact.get(id, self.unk_id) for id in gpt2_ids]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        gpt2_ids = [self.compact_to_gpt2.get(id, 0) for id in ids if id in self.compact_to_gpt2]\n",
    "        return self.base.decode(gpt2_ids)\n",
    "\n",
    "tokenizer = DucklingTokenizer(token_mapping)\n",
    "print(f\"✓ Tokenizer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_embd=N_EMBD,\n",
    "    n_head=N_HEAD,\n",
    "    n_layer=N_LAYER,\n",
    "    n_positions=N_POSITIONS,\n",
    "    n_inner=N_INNER,\n",
    "    activation_function='gelu',\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    layer_norm_epsilon=1e-5,\n",
    "    initializer_range=0.02,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✓ Model: {n_params:,} parameters\")\n",
    "\n",
    "# Access the embedding matrix (this is what we study)\n",
    "W = model.transformer.wte.weight\n",
    "print(f\"✓ Embedding matrix W: {W.shape} (vocab × hidden)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainer-header",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Uses HuggingFace Trainer with proper bf16 mixed precision.\n",
    "\n",
    "**Critical:** Use `bf16=True` in TrainingArguments, NOT `model.to(torch.bfloat16)`. The latter kills the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossTracker(TrainerCallback):\n",
    "    \"\"\"Track loss at logging steps.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.steps = []\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "            self.steps.append(state.global_step)\n",
    "\n",
    "def make_training_args(num_steps, output_dir=\"experiment\", logging_steps=10):\n",
    "    \"\"\"Create TrainingArguments with Duckling II defaults.\"\"\"\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        max_steps=num_steps,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "        bf16=True,  # CRITICAL: proper mixed precision\n",
    "        optim=\"adamw_torch\",\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=0,\n",
    "        logging_steps=logging_steps,\n",
    "        logging_first_step=True,\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"no\",\n",
    "        dataloader_num_workers=0,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "print(\"✓ Training utilities ready\")\n",
    "print(f\"  Expected speed: ~75 steps/sec\")\n",
    "print(f\"  Tokens per step: {BATCH_SIZE * N_POSITIONS:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ⬇️ YOUR EXPERIMENT HERE ⬇️\n",
    "\n",
    "Everything above is the locked-down Duckling II foundation.\n",
    "\n",
    "Add your experiment code below. You have access to:\n",
    "\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| `model` | GPT2LMHeadModel (untrained) |\n",
    "| `W` | Embedding matrix `model.transformer.wte.weight` |\n",
    "| `dataset` | Tokenized TinyStories sequences |\n",
    "| `collator` | Data collator for Trainer |\n",
    "| `tokenizer` | For encode/decode |\n",
    "| `dead_token_ids` | Set of token IDs 6144-8191 (never appear in training) |\n",
    "| `device` | 'mps', 'cuda', or 'cpu' |\n",
    "| `make_training_args()` | Helper to create TrainingArguments |\n",
    "| `LossTracker` | Callback class for tracking loss |\n",
    "\n",
    "**Example: Basic training run**\n",
    "```python\n",
    "NUM_STEPS = 10_000\n",
    "\n",
    "tracker = LossTracker()\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=make_training_args(NUM_STEPS),\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collator,\n",
    "    callbacks=[tracker],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "**Example: Record W snapshots during training**\n",
    "```python\n",
    "class WRecorder(TrainerCallback):\n",
    "    def __init__(self, record_every=100):\n",
    "        self.snapshots = []\n",
    "        self.record_every = record_every\n",
    "    \n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        if state.global_step % self.record_every == 0:\n",
    "            W = model.transformer.wte.weight.detach().cpu().clone()\n",
    "            self.snapshots.append((state.global_step, W))\n",
    "```\n",
    "\n",
    "**Example: Extract dead token embeddings**\n",
    "```python\n",
    "dead_ids = list(dead_token_ids)\n",
    "W_dead = W[dead_ids].detach()  # Shape: (2048, 128)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiment code here\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
