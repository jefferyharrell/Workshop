{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 02 Tokenizer & Dataset Preparation\n",
    "\n",
    "Build a truncated tokenizer with dead token padding for Duckling_II.\n",
    "\n",
    "**Goal:** Create a vocabulary of 8,192 tokens:\n",
    "- **Live tokens (0-6143):** Top 6,144 GPT-2 tokens by frequency in TinyStories\n",
    "- **Dead tokens (6144-8191):** 2,048 slots that never appear in training data\n",
    "\n",
    "The dead tokens are our experimental subjects—we'll watch how they move through embedding space despite never receiving direct gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parameters ===\n",
    "DATASET_NAME = \"roneneldan/TinyStories\"\n",
    "SPLIT = \"train\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Vocabulary sizes (from notebook 01 analysis)\n",
    "LIVE_TOKENS = 6144      # Top tokens by frequency\n",
    "DEAD_TOKENS = 2048      # Padding rows, never used\n",
    "TOTAL_VOCAB = 8192      # Powers of two for alignment\n",
    "\n",
    "# For building the frequency table\n",
    "SAMPLE_SIZE = 100_000   # Same as notebook 01\n",
    "\n",
    "# Context length\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequency-header",
   "metadata": {},
   "source": [
    "## Step 1: Build Token Frequency Table\n",
    "\n",
    "Reproduce the frequency analysis from notebook 01 to get our live token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-and-count",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 tokenizer loaded. Vocab size: 50,257\n",
      "\n",
      "Loading roneneldan/TinyStories...\n",
      "Dataset loaded. Total stories: 2,119,719\n",
      "Sampled 100,000 stories for frequency analysis.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd108ab52fb74acf88eb18c580348321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting tokens:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1087 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique tokens found: 17,852\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "base_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(f\"GPT-2 tokenizer loaded. Vocab size: {base_tokenizer.vocab_size:,}\")\n",
    "\n",
    "# Load dataset\n",
    "print(f\"\\nLoading {DATASET_NAME}...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=SPLIT)\n",
    "print(f\"Dataset loaded. Total stories: {len(dataset):,}\")\n",
    "\n",
    "# Sample for frequency analysis\n",
    "if SAMPLE_SIZE and SAMPLE_SIZE < len(dataset):\n",
    "    sample = dataset.shuffle(seed=RANDOM_SEED).select(range(SAMPLE_SIZE))\n",
    "    print(f\"Sampled {SAMPLE_SIZE:,} stories for frequency analysis.\")\n",
    "else:\n",
    "    sample = dataset\n",
    "\n",
    "# Count token frequencies\n",
    "token_counts = Counter()\n",
    "BATCH_SIZE = 1000\n",
    "texts = sample[\"text\"]\n",
    "n_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in tqdm(range(n_batches), desc=\"Counting tokens\"):\n",
    "    start = i * BATCH_SIZE\n",
    "    end = min(start + BATCH_SIZE, len(texts))\n",
    "    batch_encodings = base_tokenizer(texts[start:end], add_special_tokens=False)[\"input_ids\"]\n",
    "    for tokens in batch_encodings:\n",
    "        token_counts.update(tokens)\n",
    "\n",
    "print(f\"\\nUnique tokens found: {len(token_counts):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mapping-header",
   "metadata": {},
   "source": [
    "## Step 2: Build Token Mapping\n",
    "\n",
    "Create bidirectional mappings between GPT-2 token IDs and our compact vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "build-mapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 6,144 live tokens\n",
      "Dead token slots: 2,048 (indices 6144 to 8191)\n",
      "\n",
      "Mapping built:\n",
      "  GPT-2 ID 13 -> compact ID 0 ('.')\n",
      "  GPT-2 ID 44377 -> compact ID 6143 ('cking')\n"
     ]
    }
   ],
   "source": [
    "# Get top tokens by frequency\n",
    "sorted_tokens = token_counts.most_common()\n",
    "live_token_ids = [token_id for token_id, _ in sorted_tokens[:LIVE_TOKENS]]\n",
    "\n",
    "print(f\"Selected {len(live_token_ids):,} live tokens\")\n",
    "print(f\"Dead token slots: {DEAD_TOKENS:,} (indices {LIVE_TOKENS} to {TOTAL_VOCAB - 1})\")\n",
    "\n",
    "# Build mappings\n",
    "# gpt2_to_compact: GPT-2 token ID -> compact ID (0 to LIVE_TOKENS-1)\n",
    "# compact_to_gpt2: compact ID -> GPT-2 token ID\n",
    "gpt2_to_compact = {gpt2_id: compact_id for compact_id, gpt2_id in enumerate(live_token_ids)}\n",
    "compact_to_gpt2 = {compact_id: gpt2_id for compact_id, gpt2_id in enumerate(live_token_ids)}\n",
    "\n",
    "# Special token handling: use the most common token as UNK fallback\n",
    "UNK_ID = 0  # Will map unknown tokens to the most frequent token (likely '.')\n",
    "\n",
    "print(f\"\\nMapping built:\")\n",
    "print(f\"  GPT-2 ID {live_token_ids[0]} -> compact ID 0 ('{base_tokenizer.decode([live_token_ids[0]])}')\")\n",
    "print(f\"  GPT-2 ID {live_token_ids[-1]} -> compact ID {LIVE_TOKENS-1} ('{base_tokenizer.decode([live_token_ids[-1]])}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coverage-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage with 6,144 live tokens: 99.5481%\n",
      "OOV rate: 0.4519%\n"
     ]
    }
   ],
   "source": [
    "# Verify coverage\n",
    "total_tokens = sum(token_counts.values())\n",
    "covered_tokens = sum(token_counts[tid] for tid in live_token_ids)\n",
    "coverage = covered_tokens / total_tokens\n",
    "\n",
    "print(f\"Coverage with {LIVE_TOKENS:,} live tokens: {coverage:.4%}\")\n",
    "print(f\"OOV rate: {1 - coverage:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapper-header",
   "metadata": {},
   "source": [
    "## Step 3: Create Tokenizer Wrapper\n",
    "\n",
    "A wrapper class that uses GPT-2 for actual tokenization, then remaps to our compact vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wrapper-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompactTokenizer created:\n",
      "  vocab_size: 8,192\n",
      "  live tokens: 0-6143\n",
      "  dead tokens: 6144-8191\n"
     ]
    }
   ],
   "source": [
    "class CompactTokenizer:\n",
    "    \"\"\"\n",
    "    Wrapper around GPT-2 tokenizer that remaps to a compact vocabulary.\n",
    "    \n",
    "    Live tokens: 0 to (live_size - 1)\n",
    "    Dead tokens: live_size to (total_size - 1) -- never produced by encode()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_tokenizer, gpt2_to_compact, compact_to_gpt2, \n",
    "                 live_size, total_size, unk_id=0):\n",
    "        self.base = base_tokenizer\n",
    "        self.gpt2_to_compact = gpt2_to_compact\n",
    "        self.compact_to_gpt2 = compact_to_gpt2\n",
    "        self.live_size = live_size\n",
    "        self.total_size = total_size\n",
    "        self.unk_id = unk_id\n",
    "        \n",
    "        # For HuggingFace compatibility\n",
    "        self.vocab_size = total_size\n",
    "        self.pad_token_id = unk_id  # Use UNK as padding\n",
    "        self.eos_token_id = unk_id  # Simplified: no special EOS\n",
    "        self.bos_token_id = unk_id  # Simplified: no special BOS\n",
    "    \n",
    "    def encode(self, text, add_special_tokens=False):\n",
    "        \"\"\"Tokenize text and remap to compact vocabulary.\"\"\"\n",
    "        gpt2_ids = self.base.encode(text, add_special_tokens=add_special_tokens)\n",
    "        return [self.gpt2_to_compact.get(tid, self.unk_id) for tid in gpt2_ids]\n",
    "    \n",
    "    def decode(self, compact_ids):\n",
    "        \"\"\"Decode compact IDs back to text.\"\"\"\n",
    "        # Only decode live tokens; dead tokens decode to empty string\n",
    "        gpt2_ids = []\n",
    "        for cid in compact_ids:\n",
    "            if cid in self.compact_to_gpt2:\n",
    "                gpt2_ids.append(self.compact_to_gpt2[cid])\n",
    "            # Dead tokens (>= live_size) are silently skipped\n",
    "        return self.base.decode(gpt2_ids)\n",
    "    \n",
    "    def __call__(self, texts, **kwargs):\n",
    "        \"\"\"Batch tokenization for HuggingFace compatibility.\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        add_special = kwargs.get('add_special_tokens', False)\n",
    "        return {\n",
    "            'input_ids': [self.encode(t, add_special_tokens=add_special) for t in texts]\n",
    "        }\n",
    "\n",
    "# Create our tokenizer\n",
    "tokenizer = CompactTokenizer(\n",
    "    base_tokenizer=base_tokenizer,\n",
    "    gpt2_to_compact=gpt2_to_compact,\n",
    "    compact_to_gpt2=compact_to_gpt2,\n",
    "    live_size=LIVE_TOKENS,\n",
    "    total_size=TOTAL_VOCAB,\n",
    "    unk_id=UNK_ID\n",
    ")\n",
    "\n",
    "print(f\"CompactTokenizer created:\")\n",
    "print(f\"  vocab_size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"  live tokens: 0-{tokenizer.live_size - 1}\")\n",
    "print(f\"  dead tokens: {tokenizer.live_size}-{tokenizer.total_size - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-header",
   "metadata": {},
   "source": [
    "## Step 4: Test the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "test-tokenizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample story:\n",
      "--------------------------------------------------\n",
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them b...\n",
      "--------------------------------------------------\n",
      "\n",
      "Encoded to 162 tokens\n",
      "First 20 token IDs: [103, 20, 4, 6, 36, 60, 76, 27, 121, 6, 2007, 16, 9, 201, 0, 10, 166, 8, 7, 1486]\n",
      "Max token ID: 4109 (should be < 6144)\n",
      "\n",
      "Round-trip decode matches: True\n"
     ]
    }
   ],
   "source": [
    "# Test on a sample story\n",
    "test_story = dataset[0][\"text\"]\n",
    "print(\"Sample story:\")\n",
    "print(\"-\" * 50)\n",
    "print(test_story[:500] + \"...\" if len(test_story) > 500 else test_story)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Encode\n",
    "compact_ids = tokenizer.encode(test_story)\n",
    "print(f\"\\nEncoded to {len(compact_ids)} tokens\")\n",
    "print(f\"First 20 token IDs: {compact_ids[:20]}\")\n",
    "print(f\"Max token ID: {max(compact_ids)} (should be < {LIVE_TOKENS})\")\n",
    "\n",
    "# Decode\n",
    "decoded = tokenizer.decode(compact_ids)\n",
    "print(f\"\\nRound-trip decode matches: {decoded == test_story}\")\n",
    "if decoded != test_story:\n",
    "    print(f\"  (Differences due to OOV tokens mapped to UNK)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "verify-dead-tokens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying dead tokens never appear in encoded data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a35a6c5f91745f48568c9ad0078831f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking stories:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max token ID seen: 6143\n",
      "Dead token appearances: 0\n",
      "Dead tokens are truly dead ✓\n"
     ]
    }
   ],
   "source": [
    "# Verify dead tokens are truly dead\n",
    "print(\"Verifying dead tokens never appear in encoded data...\")\n",
    "\n",
    "max_token_seen = 0\n",
    "dead_token_appearances = 0\n",
    "\n",
    "# Check a sample of stories\n",
    "for i in tqdm(range(min(10000, len(dataset))), desc=\"Checking stories\"):\n",
    "    ids = tokenizer.encode(dataset[i][\"text\"])\n",
    "    if ids:\n",
    "        max_seen = max(ids)\n",
    "        if max_seen > max_token_seen:\n",
    "            max_token_seen = max_seen\n",
    "        dead_token_appearances += sum(1 for tid in ids if tid >= LIVE_TOKENS)\n",
    "\n",
    "print(f\"\\nMax token ID seen: {max_token_seen}\")\n",
    "print(f\"Dead token appearances: {dead_token_appearances}\")\n",
    "print(f\"Dead tokens are {'truly dead ✓' if dead_token_appearances == 0 else 'NOT dead ✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## Step 5: Create Dataset Pipeline\n",
    "\n",
    "Build a tokenized dataset ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tokenize-dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing full dataset (2,119,719 stories)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7a102dce63413098ce29d5dd9f1d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. 2,119,719 examples.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize a batch of examples.\"\"\"\n",
    "    return tokenizer(examples[\"text\"], add_special_tokens=False)\n",
    "\n",
    "# Tokenize the full dataset\n",
    "print(f\"Tokenizing full dataset ({len(dataset):,} stories)...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "print(f\"Tokenization complete. {len(tokenized_dataset):,} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chunk-sequences",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking into sequences of 512 tokens...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27096ee424f84042a67a819c203e336c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunking:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking complete. 921,519 sequences of 512 tokens.\n"
     ]
    }
   ],
   "source": [
    "def chunk_sequences(examples):\n",
    "    \"\"\"\n",
    "    Concatenate all sequences and split into fixed-length chunks.\n",
    "    This is the standard approach for causal LM training.\n",
    "    \"\"\"\n",
    "    # Concatenate all input_ids\n",
    "    concatenated = []\n",
    "    for ids in examples[\"input_ids\"]:\n",
    "        concatenated.extend(ids)\n",
    "    \n",
    "    # Calculate number of complete chunks\n",
    "    total_length = len(concatenated)\n",
    "    n_chunks = total_length // MAX_LENGTH\n",
    "    \n",
    "    # Split into chunks (drop remainder)\n",
    "    chunks = [\n",
    "        concatenated[i * MAX_LENGTH : (i + 1) * MAX_LENGTH]\n",
    "        for i in range(n_chunks)\n",
    "    ]\n",
    "    \n",
    "    return {\"input_ids\": chunks}\n",
    "\n",
    "# Chunk the tokenized dataset\n",
    "print(f\"Chunking into sequences of {MAX_LENGTH} tokens...\")\n",
    "chunked_dataset = tokenized_dataset.map(\n",
    "    chunk_sequences,\n",
    "    batched=True,\n",
    "    batch_size=10000,  # Process many stories at once for efficient chunking\n",
    "    desc=\"Chunking\"\n",
    ")\n",
    "print(f\"Chunking complete. {len(chunked_dataset):,} sequences of {MAX_LENGTH} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dataset-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET STATISTICS\n",
      "==================================================\n",
      "\n",
      "Sequences: 921,519\n",
      "Tokens per sequence: 512\n",
      "Total tokens: 471,817,728\n",
      "\n",
      "At batch_size=64:\n",
      "  Steps per epoch: 14,398\n",
      "  Tokens per step: 32,768\n"
     ]
    }
   ],
   "source": [
    "# Dataset statistics\n",
    "n_sequences = len(chunked_dataset)\n",
    "total_tokens = n_sequences * MAX_LENGTH\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nSequences: {n_sequences:,}\")\n",
    "print(f\"Tokens per sequence: {MAX_LENGTH}\")\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"\\nAt batch_size=64:\")\n",
    "print(f\"  Steps per epoch: {n_sequences // 64:,}\")\n",
    "print(f\"  Tokens per step: {64 * MAX_LENGTH:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Step 6: Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "save-artifacts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: token_mapping.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa23ad4ac58a4c2b8caabe3bcff3f948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/921519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: tokenized_dataset/\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path(OUTPUT_DIR)\n",
    "\n",
    "# Save token mapping\n",
    "mapping_data = {\n",
    "    \"live_tokens\": LIVE_TOKENS,\n",
    "    \"dead_tokens\": DEAD_TOKENS,\n",
    "    \"total_vocab\": TOTAL_VOCAB,\n",
    "    \"unk_id\": UNK_ID,\n",
    "    \"gpt2_to_compact\": {str(k): v for k, v in gpt2_to_compact.items()},\n",
    "    \"compact_to_gpt2\": {str(k): v for k, v in compact_to_gpt2.items()},\n",
    "    \"live_token_ids\": live_token_ids,  # Ordered by frequency\n",
    "}\n",
    "\n",
    "mapping_path = output_dir / \"token_mapping.json\"\n",
    "with open(mapping_path, \"w\") as f:\n",
    "    json.dump(mapping_data, f, indent=2)\n",
    "print(f\"Saved: {mapping_path}\")\n",
    "\n",
    "# Save chunked dataset\n",
    "dataset_path = output_dir / \"tokenized_dataset\"\n",
    "chunked_dataset.save_to_disk(str(dataset_path))\n",
    "print(f\"Saved: {dataset_path}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook created:\n",
    "\n",
    "1. **Token mapping** (`token_mapping.json`): Bidirectional mapping between GPT-2 IDs and compact vocabulary\n",
    "2. **Tokenized dataset** (`tokenized_dataset/`): Pre-chunked sequences ready for training\n",
    "\n",
    "**Key properties:**\n",
    "- Vocabulary: 8,192 total (6,144 live + 2,048 dead)\n",
    "- Dead tokens (IDs 6144-8191) never appear in training data\n",
    "- Coverage: ~99.5% of TinyStories tokens\n",
    "\n",
    "**Next step:** Training with embedding recording (notebook 03)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
