{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Goldilocks Reference Model\n",
    "\n",
    "**This is the canonical foundation for all Azimuth experiments.**\n",
    "\n",
    "Copy this notebook as your starting point. Everything here is locked down for reproducibility:\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Architecture | Rich (4L/128D/2H/256FF) |\n",
    "| Parameters | ~1.05M |\n",
    "| Vocab size | 3,988 |\n",
    "| Dead tokens | 1,914 |\n",
    "| Sequence length | 128 |\n",
    "| Batch size | 8 |\n",
    "| Dtype | bfloat16 |\n",
    "| Optimizer | AdamW (lr=1e-3) |\n",
    "| Seed | 42 |\n",
    "\n",
    "**What you get:**\n",
    "- ~100 steps/sec on M4 Pro\n",
    "- Fimbulwinter onset around step 2000-3000\n",
    "- 95%+ dead tokens frozen by step 5000\n",
    "\n",
    "**To use:** Copy this notebook, add your experiment code in the marked section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "**Do not modify these** unless you're intentionally deviating from the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# === GOLDILOCKS REFERENCE PARAMETERS ===\n# Do not modify unless intentionally deviating\n\nimport torch\n\n# Paths — reference Goldilocks data from sibling projects\nGOLDILOCKS_DATA = \"../Goldilocks/data\"\nTOKENIZER_PATH = f\"{GOLDILOCKS_DATA}/tokenizer.json\"\nTOKENS_PATH = f\"{GOLDILOCKS_DATA}/model_corpus_tokens.safetensors\"\nCENSUS_PATH = f\"{GOLDILOCKS_DATA}/token_census.json\"\n\n# Architecture: Rich\nN_LAYERS = 4\nD_MODEL = 128\nN_HEADS = 2\nD_FF = 256\nSEQ_LEN = 128\nDROPOUT = 0.0\n\n# Training\nBATCH_SIZE = 8\nLEARNING_RATE = 1e-3\nMODEL_DTYPE = torch.bfloat16\n\n# Reproducibility\nRANDOM_SEED = 42"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Imports & Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from safetensors.torch import load_file\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Device detection\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Dtype: {MODEL_DTYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Uses cached tokenized corpus for fast loading (~instant vs ~30s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"✓ Tokenizer: {vocab_size:,} tokens\")\n",
    "\n",
    "# Load cached tokenized corpus (fast!)\n",
    "tokens_data = load_file(TOKENS_PATH)\n",
    "all_tokens = tokens_data[\"tokens\"].to(torch.long)\n",
    "print(f\"✓ Corpus: {len(all_tokens):,} tokens\")\n",
    "\n",
    "# Load dead token census\n",
    "with open(CENSUS_PATH, 'r') as f:\n",
    "    census = json.load(f)\n",
    "dead_token_ids = set(census['dead_token_ids'])\n",
    "print(f\"✓ Dead tokens: {len(dead_token_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    \"\"\"Random chunks from the tokenized corpus.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokens, seq_len, num_samples=100_000):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "        max_start = len(tokens) - seq_len - 1\n",
    "        self.starts = torch.randint(0, max_start, (num_samples,))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = self.starts[idx]\n",
    "        chunk = self.tokens[start:start + self.seq_len + 1]\n",
    "        return chunk[:-1], chunk[1:]  # input, target\n",
    "\n",
    "dataset = TokenDataset(all_tokens, SEQ_LEN)\n",
    "print(f\"✓ Dataset: {len(dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "class GPT(nn.Module):\n    \"\"\"Minimal GPT — the Goldilocks reference architecture.\"\"\"\n    \n    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, seq_len, dropout=0.0):\n        super().__init__()\n        self.tok_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Embedding(seq_len, d_model)\n        self.layers = nn.ModuleList([\n            nn.TransformerEncoderLayer(\n                d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n                dropout=dropout, activation='gelu', batch_first=True, norm_first=True\n            ) for _ in range(n_layers)\n        ])\n        self.ln_f = nn.LayerNorm(d_model)\n        self.head = nn.Linear(d_model, vocab_size, bias=False)\n        self.head.weight = self.tok_emb.weight  # Weight tying\n        self.seq_len = seq_len\n        self.register_buffer('causal_mask', None)\n        \n        # Explicit initialization of token embeddings only\n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"Initialize token embeddings explicitly. No magic.\"\"\"\n        # Token embeddings (W): N(0, 0.02) via torch.randn * 0.02\n        # This is the matrix we're studying. Be explicit.\n        with torch.no_grad():\n            self.tok_emb.weight.copy_(torch.randn(self.tok_emb.weight.shape) * 0.02)\n        # pos_emb: leave as PyTorch default (not part of our investigation)\n        # head.weight: tied to tok_emb.weight, already initialized\n    \n    def forward(self, x):\n        B, T = x.shape\n        if self.causal_mask is None or self.causal_mask.shape[0] != T:\n            self.causal_mask = torch.triu(\n                torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1\n            )\n        pos = torch.arange(T, device=x.device)\n        h = self.tok_emb(x) + self.pos_emb(pos)\n        for layer in self.layers:\n            h = layer(h, src_mask=self.causal_mask, is_causal=True)\n        return self.head(self.ln_f(h))\n\n# Create model\nmodel = GPT(\n    vocab_size=vocab_size,\n    d_model=D_MODEL,\n    n_heads=N_HEADS,\n    n_layers=N_LAYERS,\n    d_ff=D_FF,\n    seq_len=SEQ_LEN,\n    dropout=DROPOUT\n).to(device).to(MODEL_DTYPE)\n\nn_params = sum(p.numel() for p in model.parameters())\nprint(f\"✓ Model: {n_params:,} parameters ({MODEL_DTYPE})\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"✓ Optimizer: AdamW (lr={LEARNING_RATE})\")\n",
    "print(f\"✓ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"✓ Tokens per step: {BATCH_SIZE * SEQ_LEN:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ⬇️ YOUR EXPERIMENT HERE ⬇️\n",
    "\n",
    "Everything above is the locked-down Goldilocks foundation.\n",
    "\n",
    "Add your experiment code below. You have access to:\n",
    "- `model` — the GPT model (bf16)\n",
    "- `optimizer` — AdamW optimizer\n",
    "- `loader` — DataLoader for training batches\n",
    "- `dataset` — the TokenDataset\n",
    "- `dead_token_ids` — set of token IDs that never appear in training\n",
    "- `tokenizer` — the HuggingFace tokenizer\n",
    "- `device` — 'mps', 'cuda', or 'cpu'\n",
    "\n",
    "**Example training loop:**\n",
    "```python\n",
    "model.train()\n",
    "loader_iter = iter(loader)\n",
    "\n",
    "for step in tqdm(range(NUM_STEPS)):\n",
    "    try:\n",
    "        x, y = next(loader_iter)\n",
    "    except StopIteration:\n",
    "        loader_iter = iter(loader)\n",
    "        x, y = next(loader_iter)\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Your instrumentation here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiment code here\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}