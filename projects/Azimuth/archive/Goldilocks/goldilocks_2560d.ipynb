{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Goldilocks 2560D\n",
    "\n",
    "**Question:** What happens when we scale Goldilocks to Qwen's hidden dimension?\n",
    "\n",
    "This is an experiment to see if certain behaviors only emerge at high dimensionality.\n",
    "We keep vocab and architecture the same, but scale D from 128 → 2560 (20×).\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Architecture | 4L/2560D/32H/10240FF |\n",
    "| Vocab size | 3,988 |\n",
    "| Dead tokens | 1,914 |\n",
    "| Sequence length | 128 |\n",
    "| Batch size | 8 |\n",
    "| Dtype | bfloat16 |\n",
    "\n",
    "---\n",
    "\n",
    "*Jeffery Harrell & Alpha, December 1, 2025*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Paths\n",
    "GOLDILOCKS_DATA = \"data\"\n",
    "TOKENIZER_PATH = f\"{GOLDILOCKS_DATA}/tokenizer.json\"\n",
    "TOKENS_PATH = f\"{GOLDILOCKS_DATA}/model_corpus_tokens.safetensors\"\n",
    "CENSUS_PATH = f\"{GOLDILOCKS_DATA}/token_census.json\"\n",
    "\n",
    "# Architecture: Qwen-scale hidden dim\n",
    "N_LAYERS = 4\n",
    "D_MODEL = 2560  # Qwen 3 4B's hidden_size\n",
    "N_HEADS = 32    # Scale heads with D (head_dim = 80)\n",
    "D_FF = 10240    # 4× D_MODEL, typical ratio\n",
    "SEQ_LEN = 128\n",
    "DROPOUT = 0.0\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-3\n",
    "MODEL_DTYPE = torch.bfloat16\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from safetensors.torch import load_file\n",
    "from tokenizers import Tokenizer\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Dtype: {MODEL_DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer: 3,988 tokens\n",
      "✓ Corpus: 34,993,926 tokens\n",
      "✓ Dead tokens: 1,914\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and data\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"✓ Tokenizer: {vocab_size:,} tokens\")\n",
    "\n",
    "tokens_data = load_file(TOKENS_PATH)\n",
    "all_tokens = tokens_data[\"tokens\"].to(torch.long)\n",
    "print(f\"✓ Corpus: {len(all_tokens):,} tokens\")\n",
    "\n",
    "with open(CENSUS_PATH, 'r') as f:\n",
    "    census = json.load(f)\n",
    "dead_token_ids = set(census['dead_token_ids'])\n",
    "\n",
    "dead_mask = torch.zeros(vocab_size, dtype=torch.bool)\n",
    "for tid in dead_token_ids:\n",
    "    dead_mask[tid] = True\n",
    "live_mask = ~dead_mask\n",
    "\n",
    "print(f\"✓ Dead tokens: {len(dead_token_ids):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset: 100,000 samples\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len, num_samples=100_000):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "        max_start = len(tokens) - seq_len - 1\n",
    "        self.starts = torch.randint(0, max_start, (num_samples,))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = self.starts[idx]\n",
    "        chunk = self.tokens[start:start + self.seq_len + 1]\n",
    "        return chunk[:-1], chunk[1:]\n",
    "\n",
    "dataset = TokenDataset(all_tokens, SEQ_LEN)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "print(f\"✓ Dataset: {len(dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model: 325,248,000 parameters (torch.bfloat16)\n",
      "  tok_emb: 10,209,280\n",
      "  pos_emb: 327,680\n"
     ]
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, seq_len, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(seq_len, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "                dropout=dropout, activation='gelu', batch_first=True, norm_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.head.weight = self.tok_emb.weight  # Weight tying\n",
    "        self.seq_len = seq_len\n",
    "        self.register_buffer('causal_mask', None)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize token embeddings explicitly. N(0, 0.02) like Qwen.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.tok_emb.weight.copy_(torch.randn(self.tok_emb.weight.shape) * 0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        if self.causal_mask is None or self.causal_mask.shape[0] != T:\n",
    "            self.causal_mask = torch.triu(\n",
    "                torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1\n",
    "            )\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        h = self.tok_emb(x) + self.pos_emb(pos)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, src_mask=self.causal_mask, is_causal=True)\n",
    "        return self.head(self.ln_f(h))\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    d_ff=D_FF,\n",
    "    seq_len=SEQ_LEN,\n",
    "    dropout=DROPOUT\n",
    ").to(device).to(MODEL_DTYPE)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✓ Model: {n_params:,} parameters ({MODEL_DTYPE})\")\n",
    "print(f\"  tok_emb: {model.tok_emb.weight.numel():,}\")\n",
    "print(f\"  pos_emb: {model.pos_emb.weight.numel():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Optimizer: AdamW (lr=0.001)\n",
      "✓ Batch size: 8\n",
      "✓ Tokens per step: 1,024\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"✓ Optimizer: AdamW (lr={LEARNING_RATE})\")\n",
    "print(f\"✓ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"✓ Tokens per step: {BATCH_SIZE * SEQ_LEN:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Speed Test\n",
    "\n",
    "Run 10 steps and time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup loss: 8.8125\n",
      "\n",
      "10 steps in 4.14s\n",
      "Speed: 2.41 steps/sec\n",
      "Final loss: 8.7500\n",
      "\n",
      "For 100 steps: ~41.4 seconds\n"
     ]
    }
   ],
   "source": [
    "NUM_STEPS = 10\n",
    "\n",
    "model.train()\n",
    "loader_iter = iter(loader)\n",
    "\n",
    "# Warmup step (first step often slower due to compilation)\n",
    "x, y = next(loader_iter)\n",
    "x, y = x.to(device), y.to(device)\n",
    "optimizer.zero_grad()\n",
    "logits = model(x)\n",
    "loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(f\"Warmup loss: {loss.item():.4f}\")\n",
    "\n",
    "# Timed run\n",
    "start_time = time.time()\n",
    "\n",
    "for step in range(NUM_STEPS):\n",
    "    x, y = next(loader_iter)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "steps_per_sec = NUM_STEPS / elapsed\n",
    "\n",
    "print(f\"\\n{NUM_STEPS} steps in {elapsed:.2f}s\")\n",
    "print(f\"Speed: {steps_per_sec:.2f} steps/sec\")\n",
    "print(f\"Final loss: {loss.item():.4f}\")\n",
    "print(f\"\\nFor 100 steps: ~{100/steps_per_sec:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Quick Sanity Check\n",
    "\n",
    "Verify dead tokens behave correctly at this scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dead gradient norm: 6.14e-06\n",
      "Live gradient norm: 2.95e-02\n",
      "Ratio (live/dead): 4810.6x\n",
      "Dead token coherence: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Get gradients from one more step\n",
    "x, y = next(loader_iter)\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "logits = model(x)\n",
    "loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "loss.backward()\n",
    "\n",
    "# Check gradient magnitudes\n",
    "grad_W = model.tok_emb.weight.grad.detach().cpu().float()\n",
    "\n",
    "dead_grads = grad_W[dead_mask]\n",
    "live_grads = grad_W[~dead_mask]\n",
    "\n",
    "dead_norm = dead_grads.norm(dim=1).mean().item()\n",
    "live_norm = live_grads.norm(dim=1).mean().item()\n",
    "\n",
    "print(f\"Dead gradient norm: {dead_norm:.2e}\")\n",
    "print(f\"Live gradient norm: {live_norm:.2e}\")\n",
    "print(f\"Ratio (live/dead): {live_norm/dead_norm:.1f}x\")\n",
    "\n",
    "# Check coherence\n",
    "if dead_grads.shape[0] > 100:\n",
    "    sample_idx = torch.randperm(dead_grads.shape[0])[:100]\n",
    "    dead_grads_sample = dead_grads[sample_idx]\n",
    "else:\n",
    "    dead_grads_sample = dead_grads\n",
    "\n",
    "dead_grads_normed = F.normalize(dead_grads_sample, dim=1)\n",
    "cos_matrix = dead_grads_normed @ dead_grads_normed.T\n",
    "n = cos_matrix.shape[0]\n",
    "triu_indices = torch.triu_indices(n, n, offset=1)\n",
    "coherence = cos_matrix[triu_indices[0], triu_indices[1]].mean().item()\n",
    "\n",
    "print(f\"Dead token coherence: {coherence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dead centroid norm: 0.209304\n",
      "Live centroid norm: 0.077209\n",
      "Angle between centroids: 43.0°\n"
     ]
    }
   ],
   "source": [
    "# Initial centroid separation\n",
    "W = model.tok_emb.weight.detach().cpu().float()\n",
    "\n",
    "centroid_dead = W[dead_mask].mean(dim=0)\n",
    "centroid_live = W[~dead_mask].mean(dim=0)\n",
    "\n",
    "cos_centroids = F.cosine_similarity(centroid_dead.unsqueeze(0), centroid_live.unsqueeze(0)).item()\n",
    "import numpy as np\n",
    "angle = np.degrees(np.arccos(np.clip(cos_centroids, -1, 1)))\n",
    "\n",
    "print(f\"Dead centroid norm: {centroid_dead.norm():.6f}\")\n",
    "print(f\"Live centroid norm: {centroid_live.norm():.6f}\")\n",
    "print(f\"Angle between centroids: {angle:.1f}°\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
