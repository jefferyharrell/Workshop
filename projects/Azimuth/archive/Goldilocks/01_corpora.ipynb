{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Corpus Preparation\n",
    "\n",
    "**Goal:** Prepare two corpora for Goldilocks experiments:\n",
    "\n",
    "1. **Tokenizer corpus** (~100MB): 50% English + 50% Thai  \n",
    "   â†’ Trains a BPE tokenizer that knows both languages equally\n",
    "\n",
    "2. **Model corpus** (~100MB): 100% English (different text from tokenizer corpus)  \n",
    "   â†’ Trains the model; Thai tokens never appear, so they're \"dead\"\n",
    "\n",
    "## Why Two Corpora?\n",
    "\n",
    "We're modeling Qwen 3 4B's frozen smoke phenomenon. Hypothesis: Qwen's tokenizer was trained on multilingual data (including Thai), but the model saw less Thai during training. Tokens the tokenizer knows but the model rarely/never saw â†’ dead tokens â†’ frozen smoke.\n",
    "\n",
    "By using different English text for tokenizer vs model training, we control for possible confounds from the tokenizer \"knowing\" the training data.\n",
    "\n",
    "## Why 50/50?\n",
    "\n",
    "We want roughly half our vocabulary to be \"dead\" tokensâ€”tokens the tokenizer knows but the model never sees. With a 4096-token vocabulary and 50/50 English/Thai in the tokenizer corpus, we expect:\n",
    "- ~2k English-derived tokens â†’ alive (appear in model corpus)\n",
    "- ~2k Thai-derived tokens â†’ dead (never appear)\n",
    "\n",
    "This gives us a nice model organism with substantial dead token population (~2,000) to study.\n",
    "\n",
    "## Source: CulturaX\n",
    "\n",
    "We use [CulturaX](https://huggingface.co/datasets/uonlp/CulturaX), a massive multilingual dataset (6.3 trillion tokens, 167 languages) built from mC4 + OSCAR with aggressive cleaning and deduplication.\n",
    "\n",
    "Each language is a separate, clean shardâ€”no cross-contamination. Perfect for our needs.\n",
    "\n",
    "## Character Filtering\n",
    "\n",
    "To keep the vocabulary small enough for meaningful BPE merges, we aggressively filter characters:\n",
    "\n",
    "- **English:** ASCII printable only (letters, digits, basic punctuation)\n",
    "- **Thai:** Thai script (U+0E00â€“U+0E7F) + shared punctuation/digits\n",
    "\n",
    "This gives us ~150-200 base characters instead of ~3,000, leaving room for ~3,900 learned merges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target sizes\n",
    "TOKENIZER_CORPUS_MB = 100.0  # Total size for tokenizer training\n",
    "ENGLISH_RATIO = 0.50         # 50% English, 50% Thai (for ~50/50 live/dead tokens)\n",
    "MODEL_CORPUS_MB = 100.0      # Size for model training (English only)\n",
    "\n",
    "# Output paths\n",
    "DATA_DIR = \"data\"\n",
    "TOKENIZER_CORPUS_PATH = f\"{DATA_DIR}/tokenizer_corpus.txt\"\n",
    "MODEL_CORPUS_PATH = f\"{DATA_DIR}/model_corpus.txt\"\n",
    "\n",
    "# Dataset: CulturaX (clean multilingual data, heavily deduplicated)\n",
    "CULTURAX_NAME = \"uonlp/CulturaX\"\n",
    "LANG_ENGLISH = \"en\"\n",
    "LANG_THAI = \"th\"\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports complete\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Ensure output directory exists\n",
    "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_until_size(dataset_name: str, lang: str, target_mb: float, skip_mb: float = 0.0) -> list[str]:\n",
    "    \"\"\"\n",
    "    Stream from CulturaX until we collect target_mb of text.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: HuggingFace dataset name (uonlp/CulturaX)\n",
    "        lang: Language code (e.g., 'en', 'th')\n",
    "        target_mb: Target size in megabytes\n",
    "        skip_mb: Skip this many MB first (for getting non-overlapping samples)\n",
    "    \n",
    "    Returns:\n",
    "        List of document strings\n",
    "    \"\"\"\n",
    "    print(f\"Streaming from {dataset_name} (lang={lang})...\")\n",
    "    if skip_mb > 0:\n",
    "        print(f\"  Skipping first {skip_mb:.1f} MB\")\n",
    "    print(f\"  Target: {target_mb:.1f} MB\")\n",
    "    \n",
    "    dataset = load_dataset(\n",
    "        dataset_name,\n",
    "        lang,\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "    \n",
    "    target_bytes = int(target_mb * 1024 * 1024)\n",
    "    skip_bytes = int(skip_mb * 1024 * 1024)\n",
    "    \n",
    "    texts = []\n",
    "    total_bytes = 0\n",
    "    skipped_bytes = 0\n",
    "    \n",
    "    for example in dataset:\n",
    "        text = example['text']\n",
    "        text_bytes = len(text.encode('utf-8'))\n",
    "        \n",
    "        # Skip phase\n",
    "        if skipped_bytes < skip_bytes:\n",
    "            skipped_bytes += text_bytes\n",
    "            continue\n",
    "        \n",
    "        # Collection phase\n",
    "        texts.append(text)\n",
    "        total_bytes += text_bytes\n",
    "        \n",
    "        if total_bytes >= target_bytes:\n",
    "            break\n",
    "    \n",
    "    actual_mb = total_bytes / (1024 * 1024)\n",
    "    print(f\"  âœ“ Collected {len(texts):,} documents ({actual_mb:.2f} MB)\")\n",
    "    \n",
    "    return texts\n",
    "\n",
    "\n",
    "def save_corpus(texts: list[str], path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Save texts to a file and return stats.\n",
    "    \"\"\"\n",
    "    combined = '\\n\\n'.join(texts)\n",
    "    \n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(combined)\n",
    "    \n",
    "    size_bytes = len(combined.encode('utf-8'))\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    \n",
    "    stats = {\n",
    "        'documents': len(texts),\n",
    "        'characters': len(combined),\n",
    "        'bytes': size_bytes,\n",
    "        'mb': size_mb,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ“ Saved to {path}\")\n",
    "    print(f\"  Documents: {stats['documents']:,}\")\n",
    "    print(f\"  Characters: {stats['characters']:,}\")\n",
    "    print(f\"  Size: {stats['mb']:.2f} MB\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character set sizes:\n",
      "  English allowed: 100 chars\n",
      "  Thai allowed: 172 chars (Thai script + shared)\n",
      "  Overlap: 42 chars\n",
      "  Total unique: 230 chars\n"
     ]
    }
   ],
   "source": [
    "# Define allowed character sets\n",
    "ASCII_PRINTABLE = set(string.printable)  # All ASCII printable chars (letters, digits, punctuation, whitespace)\n",
    "THAI_RANGE = set(chr(c) for c in range(0x0E00, 0x0E80))  # Thai script block\n",
    "\n",
    "# Shared characters allowed in Thai text (digits, basic punctuation, whitespace)\n",
    "SHARED_CHARS = set('0123456789.,;:!?\\'\\\"()-â€“â€”/\\\\@#$%&*+=<>[]{}|~ \\t\\n')\n",
    "\n",
    "# Combined allowed sets\n",
    "ENGLISH_ALLOWED = ASCII_PRINTABLE\n",
    "THAI_ALLOWED = THAI_RANGE | SHARED_CHARS\n",
    "\n",
    "print(f\"Character set sizes:\")\n",
    "print(f\"  English allowed: {len(ENGLISH_ALLOWED)} chars\")\n",
    "print(f\"  Thai allowed: {len(THAI_ALLOWED)} chars (Thai script + shared)\")\n",
    "print(f\"  Overlap: {len(ENGLISH_ALLOWED & THAI_ALLOWED)} chars\")\n",
    "print(f\"  Total unique: {len(ENGLISH_ALLOWED | THAI_ALLOWED)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English cleaning demo:\n",
      "  Before: 'Hello, world! CafÃ© rÃ©sumÃ© naÃ¯ve ä½ å¥½ ðŸŽ‰'\n",
      "  After:  'Hello, world! Caf rsum nave  '\n",
      "\n",
      "Thai cleaning demo:\n",
      "  Before: 'à¸ªà¸§à¸±à¸ªà¸”à¸µ Hello 12345 ä½ å¥½ ðŸŽ‰'\n",
      "  After:  'à¸ªà¸§à¸±à¸ªà¸”à¸µ  12345  '\n"
     ]
    }
   ],
   "source": [
    "def clean_english(text: str) -> str:\n",
    "    \"\"\"Strip all non-ASCII characters from English text.\"\"\"\n",
    "    return ''.join(c if c in ENGLISH_ALLOWED else '' for c in text)\n",
    "\n",
    "\n",
    "def clean_thai(text: str) -> str:\n",
    "    \"\"\"Keep only Thai script and shared punctuation/digits.\"\"\"\n",
    "    return ''.join(c if c in THAI_ALLOWED else '' for c in text)\n",
    "\n",
    "\n",
    "def clean_texts(texts: list[str], cleaner) -> list[str]:\n",
    "    \"\"\"Apply a cleaning function to a list of texts, dropping empty results.\"\"\"\n",
    "    cleaned = []\n",
    "    for text in texts:\n",
    "        clean = cleaner(text)\n",
    "        # Collapse multiple spaces/newlines\n",
    "        clean = re.sub(r'[ \\t]+', ' ', clean)\n",
    "        clean = re.sub(r'\\n{3,}', '\\n\\n', clean)\n",
    "        clean = clean.strip()\n",
    "        if clean:  # Only keep non-empty documents\n",
    "            cleaned.append(clean)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Demo the cleaning\n",
    "sample_en = \"Hello, world! CafÃ© rÃ©sumÃ© naÃ¯ve ä½ å¥½ ðŸŽ‰\"\n",
    "sample_th = \"à¸ªà¸§à¸±à¸ªà¸”à¸µ Hello 12345 ä½ å¥½ ðŸŽ‰\"\n",
    "\n",
    "print(f\"English cleaning demo:\")\n",
    "print(f\"  Before: {repr(sample_en)}\")\n",
    "print(f\"  After:  {repr(clean_english(sample_en))}\")\n",
    "print()\n",
    "print(f\"Thai cleaning demo:\")\n",
    "print(f\"  Before: {repr(sample_th)}\")\n",
    "print(f\"  After:  {repr(clean_thai(sample_th))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Filtering Functions\n",
    "\n",
    "We restrict each language to a small, clean character set to keep the base vocabulary small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenizer Corpus (English + Thai)\n",
    "\n",
    "50% English, 50% Thai from CulturaX's cleanly separated language shards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TOKENIZER CORPUS\n",
      "======================================================================\n",
      "\n",
      "Target composition:\n",
      "  English: 50.0 MB (50%)\n",
      "  Thai: 50.0 MB (50%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TOKENIZER CORPUS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "english_target_mb = TOKENIZER_CORPUS_MB * ENGLISH_RATIO\n",
    "thai_target_mb = TOKENIZER_CORPUS_MB * (1 - ENGLISH_RATIO)\n",
    "\n",
    "print(f\"\\nTarget composition:\")\n",
    "print(f\"  English: {english_target_mb:.1f} MB ({ENGLISH_RATIO*100:.0f}%)\")\n",
    "print(f\"  Thai: {thai_target_mb:.1f} MB ({(1-ENGLISH_RATIO)*100:.0f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming from uonlp/CulturaX (lang=en)...\n",
      "  Target: 50.0 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbdc73dbf97454a8ef6bc8efc3a147b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/3072 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Collected 14,801 documents (50.01 MB)\n",
      "\n",
      "Cleaning English texts...\n",
      "  âœ“ 14,801 â†’ 14,801 documents after cleaning\n"
     ]
    }
   ],
   "source": [
    "# Download English portion for tokenizer (from beginning of CulturaX English)\n",
    "tokenizer_english_raw = stream_until_size(\n",
    "    CULTURAX_NAME, \n",
    "    LANG_ENGLISH, \n",
    "    english_target_mb,\n",
    "    skip_mb=0.0  # Start from beginning\n",
    ")\n",
    "\n",
    "# Clean: ASCII only\n",
    "print(f\"\\nCleaning English texts...\")\n",
    "tokenizer_english = clean_texts(tokenizer_english_raw, clean_english)\n",
    "print(f\"  âœ“ {len(tokenizer_english_raw):,} â†’ {len(tokenizer_english):,} documents after cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming from uonlp/CulturaX (lang=th)...\n",
      "  Target: 50.0 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0084405d67264d9bb73fe35f4a5c2314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Collected 7,105 documents (50.03 MB)\n",
      "\n",
      "Cleaning Thai texts...\n",
      "  âœ“ 7,105 â†’ 7,105 documents after cleaning\n"
     ]
    }
   ],
   "source": [
    "# Download Thai portion for tokenizer\n",
    "tokenizer_thai_raw = stream_until_size(\n",
    "    CULTURAX_NAME,\n",
    "    LANG_THAI,\n",
    "    thai_target_mb\n",
    ")\n",
    "\n",
    "# Clean: Thai script + shared punctuation only\n",
    "print(f\"\\nCleaning Thai texts...\")\n",
    "tokenizer_thai = clean_texts(tokenizer_thai_raw, clean_thai)\n",
    "print(f\"  âœ“ {len(tokenizer_thai_raw):,} â†’ {len(tokenizer_thai):,} documents after cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined: 21,906 documents\n",
      "  English: 14,801\n",
      "  Thai: 7,105\n"
     ]
    }
   ],
   "source": [
    "# Combine and shuffle\n",
    "tokenizer_texts = tokenizer_english + tokenizer_thai\n",
    "random.shuffle(tokenizer_texts)\n",
    "\n",
    "print(f\"\\nCombined: {len(tokenizer_texts):,} documents\")\n",
    "print(f\"  English: {len(tokenizer_english):,}\")\n",
    "print(f\"  Thai: {len(tokenizer_thai):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Saved to data/tokenizer_corpus.txt\n",
      "  Documents: 21,906\n",
      "  Characters: 70,278,142\n",
      "  Size: 98.27 MB\n"
     ]
    }
   ],
   "source": [
    "# Save tokenizer corpus\n",
    "tokenizer_stats = save_corpus(tokenizer_texts, TOKENIZER_CORPUS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Corpus (English Only)\n",
    "\n",
    "100% English from CulturaX, but from a *different* portion than the tokenizer corpus (we skip ahead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL CORPUS\n",
      "======================================================================\n",
      "\n",
      "Target: 100.0 MB English\n",
      "Skipping first 55.0 MB to avoid overlap with tokenizer corpus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL CORPUS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Skip past the English we used for tokenizer corpus to get non-overlapping text\n",
    "skip_mb = english_target_mb * 1.1  # Skip a bit extra to be safe\n",
    "\n",
    "print(f\"\\nTarget: {MODEL_CORPUS_MB:.1f} MB English\")\n",
    "print(f\"Skipping first {skip_mb:.1f} MB to avoid overlap with tokenizer corpus\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming from uonlp/CulturaX (lang=en)...\n",
      "  Skipping first 55.0 MB\n",
      "  Target: 100.0 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738d3e8469434af98d07489954081602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/3072 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Collected 29,782 documents (100.01 MB)\n",
      "\n",
      "Cleaning model corpus...\n",
      "  âœ“ 29,782 â†’ 29,782 documents after cleaning\n"
     ]
    }
   ],
   "source": [
    "# Download English portion for model (from later in CulturaX, no overlap with tokenizer)\n",
    "model_texts_raw = stream_until_size(\n",
    "    CULTURAX_NAME,\n",
    "    LANG_ENGLISH,\n",
    "    MODEL_CORPUS_MB,\n",
    "    skip_mb=skip_mb\n",
    ")\n",
    "\n",
    "# Clean: ASCII only (this also removes any Thai contamination!)\n",
    "print(f\"\\nCleaning model corpus...\")\n",
    "model_texts = clean_texts(model_texts_raw, clean_english)\n",
    "print(f\"  âœ“ {len(model_texts_raw):,} â†’ {len(model_texts):,} documents after cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Saved to data/model_corpus.txt\n",
      "  Documents: 29,782\n",
      "  Characters: 104,320,221\n",
      "  Size: 99.49 MB\n"
     ]
    }
   ],
   "source": [
    "# Save model corpus\n",
    "model_stats = save_corpus(model_texts, MODEL_CORPUS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CORPUS PREPARATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Tokenizer corpus: data/tokenizer_corpus.txt\n",
      "  Size: 98.27 MB\n",
      "  Documents: 21,906\n",
      "  Composition: 50% English, 50% Thai\n",
      "\n",
      "Model corpus: data/model_corpus.txt\n",
      "  Size: 99.49 MB\n",
      "  Documents: 29,782\n",
      "  Composition: 100% English (no overlap with tokenizer corpus)\n",
      "\n",
      "Next steps:\n",
      "  â†’ 02_tokenizer.ipynb: Train BPE tokenizer on tokenizer corpus\n",
      "  â†’ Thai tokens will be dead when model trains on English-only corpus\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORPUS PREPARATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTokenizer corpus: {TOKENIZER_CORPUS_PATH}\")\n",
    "print(f\"  Size: {tokenizer_stats['mb']:.2f} MB\")\n",
    "print(f\"  Documents: {tokenizer_stats['documents']:,}\")\n",
    "print(f\"  Composition: {ENGLISH_RATIO*100:.0f}% English, {(1-ENGLISH_RATIO)*100:.0f}% Thai\")\n",
    "\n",
    "print(f\"\\nModel corpus: {MODEL_CORPUS_PATH}\")\n",
    "print(f\"  Size: {model_stats['mb']:.2f} MB\")\n",
    "print(f\"  Documents: {model_stats['documents']:,}\")\n",
    "print(f\"  Composition: 100% English (no overlap with tokenizer corpus)\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  â†’ 02_tokenizer.ipynb: Train BPE tokenizer on tokenizer corpus\")\n",
    "print(f\"  â†’ Thai tokens will be dead when model trains on English-only corpus\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check: Language Purity\n",
    "\n",
    "CulturaX should give us clean separation. Let's verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer corpus composition:\n",
      "  Thai characters: 16,380,343\n",
      "  Total characters: 70,278,142\n",
      "  Thai %: 23.31%\n",
      "  Unique characters: 182  â† THIS IS THE KEY NUMBER\n",
      "\n",
      "Sample cleaned English (first doc, first 150 chars):\n",
      "  DOT Announces 2008 Exploration Program - Redorbit\n",
      "CALGARY, ALBERTA--(Marketwire - July 31, 2008) - DOT Resources Ltd. (TSX VENTURE:DOT) (\"DOT\" or the ...\n",
      "\n",
      "Sample cleaned Thai (first doc, first 150 chars):\n",
      "  à¹‚à¸•à¹Šà¸°à¸šà¸­à¸¥ à¸à¸±à¸šà¸à¸²à¸£à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹à¸—à¸‡à¸šà¸­à¸¥à¸­à¸­à¸™à¹„à¸¥à¸™à¹Œ - à¸šà¸²à¸„à¸²à¸£à¹ˆà¸²à¸­à¸­à¸™à¹„à¸¥à¸™à¹Œ\n",
      "à¹‚à¸•à¹Šà¸°à¸šà¸­à¸¥ à¸à¸±à¸šà¸à¸²à¸£à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹à¸—à¸‡à¸šà¸­à¸¥à¸­à¸­à¸™à¹„à¸¥à¸™à¹Œ\n",
      " à¸à¸¸à¸¡à¸ à¸²à¸žà¸±à¸™à¸˜à¹Œ 11, 2019\n",
      "à¹‚à¸•à¹Šà¸°à¸šà¸­à¸¥ à¸à¸²à¸£à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹à¸—à¸‡à¸šà¸­à¸¥à¸­à¸­à¸™à¹„à¸¥à¸™à¹Œ à¸ˆà¸°à¹€à¸£à¸´à¹ˆà¸¡...\n"
     ]
    }
   ],
   "source": [
    "def count_thai_chars(text: str) -> int:\n",
    "    \"\"\"Count characters in Thai Unicode range (U+0E00 to U+0E7F).\"\"\"\n",
    "    return sum(1 for c in text if 0x0E00 <= ord(c) <= 0x0E7F)\n",
    "\n",
    "# Check tokenizer corpus\n",
    "with open(TOKENIZER_CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    tokenizer_text = f.read()\n",
    "\n",
    "thai_chars = count_thai_chars(tokenizer_text)\n",
    "total_chars = len(tokenizer_text)\n",
    "unique_chars = set(tokenizer_text)\n",
    "\n",
    "print(f\"Tokenizer corpus composition:\")\n",
    "print(f\"  Thai characters: {thai_chars:,}\")\n",
    "print(f\"  Total characters: {total_chars:,}\")\n",
    "print(f\"  Thai %: {100*thai_chars/total_chars:.2f}%\")\n",
    "print(f\"  Unique characters: {len(unique_chars):,}  â† THIS IS THE KEY NUMBER\")\n",
    "\n",
    "# Show samples of cleaned text\n",
    "print(f\"\\nSample cleaned English (first doc, first 150 chars):\")\n",
    "print(f\"  {tokenizer_english[0][:150]}...\")\n",
    "print(f\"\\nSample cleaned Thai (first doc, first 150 chars):\")\n",
    "print(f\"  {tokenizer_thai[0][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model corpus check:\n",
      "  Unique characters: 96\n",
      "  Thai characters: 0\n",
      "  âœ“ CLEAN: No Thai in model corpus!\n",
      "\n",
      "======================================================================\n",
      "CHARACTER SET SUMMARY\n",
      "======================================================================\n",
      "Tokenizer corpus: 182 unique chars\n",
      "Model corpus: 96 unique chars\n",
      "BPE will have ~3,914 slots for learned merges\n"
     ]
    }
   ],
   "source": [
    "# Verify model corpus has NO Thai (this is the critical check!)\n",
    "with open(MODEL_CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    model_text = f.read()\n",
    "\n",
    "model_thai_chars = count_thai_chars(model_text)\n",
    "model_unique_chars = set(model_text)\n",
    "\n",
    "print(f\"Model corpus check:\")\n",
    "print(f\"  Unique characters: {len(model_unique_chars):,}\")\n",
    "print(f\"  Thai characters: {model_thai_chars:,}\")\n",
    "\n",
    "if model_thai_chars == 0:\n",
    "    print(f\"  âœ“ CLEAN: No Thai in model corpus!\")\n",
    "else:\n",
    "    print(f\"  âš  CONTAMINATED: {model_thai_chars} Thai characters found\")\n",
    "    \n",
    "# Final verdict\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"CHARACTER SET SUMMARY\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Tokenizer corpus: {len(unique_chars):,} unique chars\")\n",
    "print(f\"Model corpus: {len(model_unique_chars):,} unique chars\")\n",
    "print(f\"BPE will have ~{4096 - len(unique_chars):,} slots for learned merges\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
