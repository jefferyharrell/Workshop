{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stamp 04: Adam Math at t=1\n",
    "\n",
    "**Goal:** Work through the Adam update equation at t=1 and confirm (or refute) that bias correction causes the update to reduce to -η · sign(g).\n",
    "\n",
    "No training, no model. Just math.\n",
    "\n",
    "---\n",
    "\n",
    "*Jeffery Harrell & Alpha, December 1, 2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Adam Update Rule\n",
    "\n",
    "Standard Adam (Kingma & Ba, 2014):\n",
    "\n",
    "**Given:**\n",
    "- $g_t$ = gradient at step $t$\n",
    "- $\\eta$ = learning rate\n",
    "- $\\beta_1$ = first moment decay (default: 0.9)\n",
    "- $\\beta_2$ = second moment decay (default: 0.999)\n",
    "- $\\epsilon$ = numerical stability term (default: 1e-8)\n",
    "\n",
    "**Update equations:**\n",
    "\n",
    "1. First moment estimate (momentum):\n",
    "$$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t$$\n",
    "\n",
    "2. Second moment estimate (RMSprop-like):\n",
    "$$v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2$$\n",
    "\n",
    "3. Bias-corrected estimates:\n",
    "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "$$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "\n",
    "4. Parameter update:\n",
    "$$\\theta_t = \\theta_{t-1} - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At t=1\n",
    "\n",
    "**Initial conditions:** $m_0 = 0$, $v_0 = 0$\n",
    "\n",
    "### Step 1: Compute $m_1$ and $v_1$\n",
    "\n",
    "$$m_1 = \\beta_1 \\cdot 0 + (1 - \\beta_1) \\cdot g_1 = (1 - \\beta_1) \\cdot g_1$$\n",
    "\n",
    "$$v_1 = \\beta_2 \\cdot 0 + (1 - \\beta_2) \\cdot g_1^2 = (1 - \\beta_2) \\cdot g_1^2$$\n",
    "\n",
    "### Step 2: Bias correction\n",
    "\n",
    "$$\\hat{m}_1 = \\frac{(1 - \\beta_1) \\cdot g_1}{1 - \\beta_1^1} = \\frac{(1 - \\beta_1) \\cdot g_1}{1 - \\beta_1} = g_1$$\n",
    "\n",
    "$$\\hat{v}_1 = \\frac{(1 - \\beta_2) \\cdot g_1^2}{1 - \\beta_2^1} = \\frac{(1 - \\beta_2) \\cdot g_1^2}{1 - \\beta_2} = g_1^2$$\n",
    "\n",
    "**The bias correction terms cancel perfectly at t=1!**\n",
    "\n",
    "### Step 3: The update\n",
    "\n",
    "$$\\Delta\\theta_1 = -\\eta \\cdot \\frac{\\hat{m}_1}{\\sqrt{\\hat{v}_1} + \\epsilon} = -\\eta \\cdot \\frac{g_1}{\\sqrt{g_1^2} + \\epsilon} = -\\eta \\cdot \\frac{g_1}{|g_1| + \\epsilon}$$\n",
    "\n",
    "If $|g_1| \\gg \\epsilon$:\n",
    "\n",
    "$$\\Delta\\theta_1 \\approx -\\eta \\cdot \\frac{g_1}{|g_1|} = -\\eta \\cdot \\text{sign}(g_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion (Algebraic)\n",
    "\n",
    "**Yes, the claim is correct.** At t=1, with $m_0 = v_0 = 0$:\n",
    "\n",
    "$$\\boxed{\\Delta\\theta_1 = -\\eta \\cdot \\text{sign}(g_1)}$$\n",
    "\n",
    "(assuming $|g_1| \\gg \\epsilon$)\n",
    "\n",
    "This means:\n",
    "1. The *magnitude* of the gradient doesn't matter at t=1\n",
    "2. Only the *sign* matters\n",
    "3. Every parameter moves by exactly $\\eta$ (in absolute value)\n",
    "4. This is true regardless of whether $g_1$ is 1e-10 or 1e+10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait. Let's check the ε regime.\n",
    "\n",
    "What if $|g_1|$ is *not* much greater than $\\epsilon$?\n",
    "\n",
    "With $\\epsilon = 10^{-8}$ (PyTorch default for Adam):\n",
    "\n",
    "| $|g_1|$ | $|g_1| / (|g_1| + \\epsilon)$ | Effective multiplier |\n",
    "|---------|------------------------------|----------------------|\n",
    "| 1e-2 | 0.999999 | ≈ 1 |\n",
    "| 1e-4 | 0.9999 | ≈ 1 |\n",
    "| 1e-6 | 0.99 | ≈ 1 |\n",
    "| 1e-8 | 0.5 | 0.5 |\n",
    "| 1e-10 | 0.0099 | ≈ 0.01 |\n",
    "| 1e-12 | 0.0001 | ≈ 0 |\n",
    "\n",
    "**Oh.**\n",
    "\n",
    "If $|g_1| \\approx \\epsilon$, the update is halved.\n",
    "\n",
    "If $|g_1| \\ll \\epsilon$, the update approaches:\n",
    "\n",
    "$$\\Delta\\theta_1 \\approx -\\eta \\cdot \\frac{g_1}{\\epsilon} = -\\frac{\\eta}{\\epsilon} \\cdot g_1$$\n",
    "\n",
    "This is just scaled gradient descent with a huge learning rate $\\eta/\\epsilon = 10^{-3}/10^{-8} = 10^5$.\n",
    "\n",
    "But wait—$g_1$ is tiny, so the product might still be small..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compute actual numbers\n",
    "\n",
    "From our sanity check:\n",
    "- Dead token gradient norm at step 1: **1.79e-10**\n",
    "- Live token gradient norm at step 1: **7.65e-3**\n",
    "\n",
    "With $\\eta = 10^{-3}$, $\\epsilon = 10^{-8}$:\n",
    "\n",
    "### Live tokens:\n",
    "$$|g| = 7.65 \\times 10^{-3} \\gg \\epsilon$$\n",
    "$$|\\Delta\\theta| \\approx \\eta = 10^{-3}$$\n",
    "\n",
    "### Dead tokens:\n",
    "$$|g| = 1.79 \\times 10^{-10} \\ll \\epsilon$$\n",
    "$$|\\Delta\\theta| \\approx \\eta \\cdot \\frac{|g|}{\\epsilon} = 10^{-3} \\cdot \\frac{1.79 \\times 10^{-10}}{10^{-8}} = 10^{-3} \\cdot 1.79 \\times 10^{-2} = 1.79 \\times 10^{-5}$$\n",
    "\n",
    "So dead tokens should move by about **1.79e-5** at step 1, not **1e-3** like live tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Real Question\n",
    "\n",
    "The math shows that if $|g| \\ll \\epsilon$, the update is proportional to $g$, not $\\text{sign}(g)$.\n",
    "\n",
    "But **why is the dead token gradient so small in the first place?**\n",
    "\n",
    "Theory says:\n",
    "$$\\frac{\\partial L}{\\partial W_i} = p_i \\cdot h$$\n",
    "\n",
    "where $p_i$ is the softmax probability assigned to token $i$, and $h$ is the hidden state.\n",
    "\n",
    "For dead tokens, $p_i$ should be approximately $1/V$ (uniform) when the model is untrained, giving:\n",
    "$$|g_{dead}| \\approx \\frac{1}{V} \\cdot |h| = \\frac{1}{3988} \\cdot |h|$$\n",
    "\n",
    "With $|h| \\approx 11$ (from our h_mean norm), we'd expect:\n",
    "$$|g_{dead}| \\approx \\frac{11}{3988} \\approx 2.8 \\times 10^{-3}$$\n",
    "\n",
    "But we measured **1.79e-10**. That's 7 orders of magnitude smaller!\n",
    "\n",
    "**The mystery isn't in Adam. The mystery is in the gradient computation itself.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam update at t=1 for various gradient magnitudes:\n",
      "============================================================\n",
      "         |g| |         |Δθ| |       |Δθ|/η | Notes\n",
      "------------------------------------------------------------\n",
      "    1.00e-02 |     1.00e-03 |       1.0000 | \n",
      "    1.00e-04 |     1.00e-03 |       0.9999 | \n",
      "    1.00e-06 |     9.90e-04 |       0.9901 | \n",
      "    1.00e-08 |     5.00e-04 |       0.5000 | \n",
      "    1.00e-10 |     9.90e-06 |       0.0099 | \n",
      "    1.00e-12 |     1.00e-07 |       0.0001 | \n",
      "    7.65e-03 |     1.00e-03 |       1.0000 | ← Live tokens\n",
      "    1.79e-10 |     1.76e-05 |       0.0176 | ← Dead tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Adam parameters (PyTorch defaults)\n",
    "eta = 1e-3      # learning rate\n",
    "beta1 = 0.9     # first moment decay\n",
    "beta2 = 0.999   # second moment decay  \n",
    "eps = 1e-8      # numerical stability\n",
    "\n",
    "# At t=1, with m_0 = v_0 = 0\n",
    "def adam_update_t1(g):\n",
    "    \"\"\"Compute Adam update at t=1 for gradient g.\"\"\"\n",
    "    # Raw moments\n",
    "    m1 = (1 - beta1) * g\n",
    "    v1 = (1 - beta2) * g**2\n",
    "    \n",
    "    # Bias correction at t=1\n",
    "    m1_hat = m1 / (1 - beta1**1)  # = g\n",
    "    v1_hat = v1 / (1 - beta2**1)  # = g^2\n",
    "    \n",
    "    # Update\n",
    "    delta = -eta * m1_hat / (np.sqrt(v1_hat) + eps)\n",
    "    return delta\n",
    "\n",
    "print(\"Adam update at t=1 for various gradient magnitudes:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'|g|':>12} | {'|Δθ|':>12} | {'|Δθ|/η':>12} | Notes\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for g in [1e-2, 1e-4, 1e-6, 1e-8, 1e-10, 1e-12, 7.65e-3, 1.79e-10]:\n",
    "    delta = adam_update_t1(g)\n",
    "    ratio = abs(delta) / eta\n",
    "    note = \"\"\n",
    "    if g == 7.65e-3:\n",
    "        note = \"← Live tokens\"\n",
    "    elif g == 1.79e-10:\n",
    "        note = \"← Dead tokens\"\n",
    "    print(f\"{g:>12.2e} | {abs(delta):>12.2e} | {ratio:>12.4f} | {note}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected dead token gradient (uniform softmax theory):\n",
      "  p_dead = 1/V = 1/3988 = 2.51e-04\n",
      "  |h| ≈ 11.0\n",
      "  |g_dead| = p_dead × |h| ≈ 2.76e-03\n",
      "\n",
      "Observed dead token gradient: 1.79e-10\n",
      "Ratio (expected/observed): 1.54e+07\n"
     ]
    }
   ],
   "source": [
    "# What gradient would we EXPECT for dead tokens?\n",
    "V = 3988  # vocab size\n",
    "h_norm = 11.0  # approximate h_mean norm at step 1\n",
    "\n",
    "# If softmax is uniform, p_i = 1/V for all tokens\n",
    "p_uniform = 1 / V\n",
    "expected_grad = p_uniform * h_norm\n",
    "\n",
    "print(f\"Expected dead token gradient (uniform softmax theory):\")\n",
    "print(f\"  p_dead = 1/V = 1/{V} = {p_uniform:.2e}\")\n",
    "print(f\"  |h| ≈ {h_norm}\")\n",
    "print(f\"  |g_dead| = p_dead × |h| ≈ {expected_grad:.2e}\")\n",
    "print()\n",
    "print(f\"Observed dead token gradient: 1.79e-10\")\n",
    "print(f\"Ratio (expected/observed): {expected_grad / 1.79e-10:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. **The Adam math is correct.** At t=1, bias correction cancels and $\\Delta\\theta = -\\eta \\cdot g / (|g| + \\epsilon)$.\n",
    "\n",
    "2. **For large gradients** ($|g| \\gg \\epsilon$), this reduces to $-\\eta \\cdot \\text{sign}(g)$.\n",
    "\n",
    "3. **For tiny gradients** ($|g| \\ll \\epsilon$), this reduces to $-\\eta \\cdot g / \\epsilon$, which is proportional to $g$.\n",
    "\n",
    "4. **The mystery is NOT in Adam.** Adam would happily move dead tokens if they had reasonable gradients.\n",
    "\n",
    "5. **The mystery is in the gradient itself.** Dead tokens get gradients of 1e-10 when theory predicts ~1e-3. That's a 7 order-of-magnitude discrepancy.\n",
    "\n",
    "**Next question:** Why is $\\partial L / \\partial W_{dead}$ so small? What's happening in the forward/backward pass that makes dead tokens invisible to the gradient?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
