{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stamp 05: Gradient by Hand\n",
    "\n",
    "**Question:** Is PyTorch computing the gradient correctly, or is something wrong with our setup?\n",
    "\n",
    "We'll manually compute the gradient for the unembedding matrix and compare to what PyTorch gives us.\n",
    "\n",
    "---\n",
    "\n",
    "*Jeffery Harrell & Alpha, December 1, 2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Math\n",
    "\n",
    "For a single position in the sequence:\n",
    "\n",
    "1. **Forward pass:**\n",
    "   - Hidden state: $h \\in \\mathbb{R}^D$ (after final LayerNorm)\n",
    "   - Logits: $z = W h$ where $W \\in \\mathbb{R}^{V \\times D}$\n",
    "   - Probabilities: $p = \\text{softmax}(z)$\n",
    "   - Loss: $L = -\\log(p_y)$ where $y$ is the target token\n",
    "\n",
    "2. **Backward pass:**\n",
    "   - $\\frac{\\partial L}{\\partial z_i} = p_i - \\mathbb{1}[i = y]$\n",
    "   - $\\frac{\\partial L}{\\partial W_i} = \\frac{\\partial L}{\\partial z_i} \\cdot h^T = (p_i - \\mathbb{1}[i = y]) \\cdot h$\n",
    "\n",
    "For a dead token $i$ (never the target):\n",
    "$$\\frac{\\partial L}{\\partial W_i} = p_i \\cdot h$$\n",
    "\n",
    "Summed over a batch of B×T positions:\n",
    "$$\\frac{\\partial L}{\\partial W_i} = \\sum_{b,t} p_i^{(b,t)} \\cdot h^{(b,t)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from safetensors.torch import load_file\n",
    "from tokenizers import Tokenizer\n",
    "import json\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 3988, Dead: 1914\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "GOLDILOCKS_DATA = \"../Goldilocks/data\"\n",
    "tokenizer = Tokenizer.from_file(f\"{GOLDILOCKS_DATA}/tokenizer.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "tokens_data = load_file(f\"{GOLDILOCKS_DATA}/model_corpus_tokens.safetensors\")\n",
    "all_tokens = tokens_data[\"tokens\"].to(torch.long)\n",
    "\n",
    "with open(f\"{GOLDILOCKS_DATA}/token_census.json\", 'r') as f:\n",
    "    census = json.load(f)\n",
    "dead_token_ids = set(census['dead_token_ids'])\n",
    "\n",
    "dead_mask = torch.zeros(vocab_size, dtype=torch.bool)\n",
    "for tid in dead_token_ids:\n",
    "    dead_mask[tid] = True\n",
    "\n",
    "print(f\"Vocab: {vocab_size}, Dead: {len(dead_token_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready (tok_emb explicit: torch.randn * 0.02)\n"
     ]
    }
   ],
   "source": [
    "# Minimal model setup\n",
    "D_MODEL = 128\n",
    "N_LAYERS = 4\n",
    "N_HEADS = 2\n",
    "D_FF = 256\n",
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, seq_len):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(seq_len, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "                dropout=0.0, activation='gelu', batch_first=True, norm_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.head.weight = self.tok_emb.weight  # Weight tying!\n",
    "        self.seq_len = seq_len\n",
    "        self.register_buffer('causal_mask', None)\n",
    "        \n",
    "        # Explicit initialization of token embeddings only\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize token embeddings explicitly. No magic.\"\"\"\n",
    "        # Token embeddings (W): N(0, 0.02) via torch.randn * 0.02\n",
    "        with torch.no_grad():\n",
    "            self.tok_emb.weight.copy_(torch.randn(self.tok_emb.weight.shape) * 0.02)\n",
    "        # pos_emb: leave as PyTorch default\n",
    "    \n",
    "    def forward(self, x, return_h=False):\n",
    "        B, T = x.shape\n",
    "        if self.causal_mask is None or self.causal_mask.shape[0] != T:\n",
    "            self.causal_mask = torch.triu(\n",
    "                torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1\n",
    "            )\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        h = self.tok_emb(x) + self.pos_emb(pos)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, src_mask=self.causal_mask, is_causal=True)\n",
    "        h = self.ln_f(h)\n",
    "        logits = self.head(h)\n",
    "        if return_h:\n",
    "            return logits, h\n",
    "        return logits\n",
    "\n",
    "model = GPT(vocab_size, D_MODEL, N_HEADS, N_LAYERS, D_FF, SEQ_LEN).to(device).to(torch.bfloat16)\n",
    "print(f\"Model ready (tok_emb explicit: torch.randn * 0.02)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get One Batch and Compute Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([8, 128])\n",
      "y shape: torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "# Get a batch\n",
    "start_idx = 0\n",
    "x_list = []\n",
    "y_list = []\n",
    "for i in range(BATCH_SIZE):\n",
    "    chunk = all_tokens[start_idx + i * 1000 : start_idx + i * 1000 + SEQ_LEN + 1]\n",
    "    x_list.append(chunk[:-1])\n",
    "    y_list.append(chunk[1:])\n",
    "\n",
    "x = torch.stack(x_list).to(device)  # [B, T]\n",
    "y = torch.stack(y_list).to(device)  # [B, T]\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([8, 128, 3988])\n",
      "h shape: torch.Size([8, 128, 128])\n",
      "h norm (mean): 11.3137\n"
     ]
    }
   ],
   "source": [
    "# Forward pass, capturing h\n",
    "model.eval()  # No dropout anyway, but be explicit\n",
    "with torch.no_grad():\n",
    "    logits, h = model(x, return_h=True)\n",
    "\n",
    "print(f\"logits shape: {logits.shape}\")  # [B, T, V]\n",
    "print(f\"h shape: {h.shape}\")            # [B, T, D]\n",
    "print(f\"h norm (mean): {h.float().norm(dim=-1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs shape: torch.Size([8, 128, 3988])\n",
      "probs sum (should be 1): 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Compute softmax probabilities\n",
    "probs = F.softmax(logits.float(), dim=-1)  # [B, T, V]\n",
    "\n",
    "print(f\"probs shape: {probs.shape}\")\n",
    "print(f\"probs sum (should be 1): {probs[0, 0, :].sum():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL_dz shape: torch.Size([8, 128, 3988])\n"
     ]
    }
   ],
   "source": [
    "# For cross-entropy loss, dL/dz = p - one_hot(y)\n",
    "# Create one-hot targets\n",
    "one_hot_y = F.one_hot(y, num_classes=vocab_size).float()  # [B, T, V]\n",
    "\n",
    "# dL/dz for each position\n",
    "dL_dz = probs - one_hot_y  # [B, T, V]\n",
    "\n",
    "print(f\"dL_dz shape: {dL_dz.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual gradient shape: torch.Size([3988, 128])\n",
      "Manual gradient norm (all): 0.386533\n"
     ]
    }
   ],
   "source": [
    "# dL/dW[i] = sum over (b,t) of dL/dz[b,t,i] * h[b,t]\n",
    "# This is: dL_dz.transpose(-1,-2) @ h, summed appropriately\n",
    "# Or equivalently: einsum\n",
    "\n",
    "h_float = h.float()  # [B, T, D]\n",
    "\n",
    "# Manual gradient: for each vocab token i, gradient is sum of (dL/dz_i * h) over all positions\n",
    "# dL_dz is [B, T, V], h is [B, T, D]\n",
    "# We want [V, D]\n",
    "\n",
    "dL_dW_manual = torch.einsum('btv,btd->vd', dL_dz, h_float)  # [V, D]\n",
    "\n",
    "# Normalize by number of positions (B*T) to match PyTorch's mean reduction\n",
    "num_positions = x.shape[0] * x.shape[1]\n",
    "dL_dW_manual = dL_dW_manual / num_positions\n",
    "\n",
    "print(f\"Manual gradient shape: {dL_dW_manual.shape}\")\n",
    "print(f\"Manual gradient norm (all): {dL_dW_manual.norm():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual dead gradient mean norm: 6.414620e-04\n",
      "Manual live gradient mean norm: 4.181137e-03\n"
     ]
    }
   ],
   "source": [
    "# Check dead vs live\n",
    "dead_mask_cpu = dead_mask.cpu()\n",
    "\n",
    "manual_dead_grad = dL_dW_manual[dead_mask_cpu]\n",
    "manual_live_grad = dL_dW_manual[~dead_mask_cpu]\n",
    "\n",
    "print(f\"Manual dead gradient mean norm: {manual_dead_grad.norm(dim=1).mean():.6e}\")\n",
    "print(f\"Manual live gradient mean norm: {manual_live_grad.norm(dim=1).mean():.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch gradient shape: torch.Size([3988, 128])\n",
      "PyTorch gradient norm (all): 0.386510\n"
     ]
    }
   ],
   "source": [
    "# Now let PyTorch compute it\n",
    "model.train()\n",
    "model.zero_grad()\n",
    "\n",
    "logits_pt = model(x)  # [B, T, V]\n",
    "loss = F.cross_entropy(logits_pt.view(-1, vocab_size), y.view(-1))\n",
    "loss.backward()\n",
    "\n",
    "# Get the gradient\n",
    "dL_dW_pytorch = model.tok_emb.weight.grad.cpu().float()  # [V, D]\n",
    "\n",
    "print(f\"PyTorch gradient shape: {dL_dW_pytorch.shape}\")\n",
    "print(f\"PyTorch gradient norm (all): {dL_dW_pytorch.norm():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch dead gradient mean norm: 6.414803e-04\n",
      "PyTorch live gradient mean norm: 4.182540e-03\n"
     ]
    }
   ],
   "source": [
    "pytorch_dead_grad = dL_dW_pytorch[dead_mask_cpu]\n",
    "pytorch_live_grad = dL_dW_pytorch[~dead_mask_cpu]\n",
    "\n",
    "print(f\"PyTorch dead gradient mean norm: {pytorch_dead_grad.norm(dim=1).mean():.6e}\")\n",
    "print(f\"PyTorch live gradient mean norm: {pytorch_live_grad.norm(dim=1).mean():.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARISON: Manual vs PyTorch\n",
      "============================================================\n",
      "\n",
      "Dead token gradients:\n",
      "  Manual:  6.414620e-04\n",
      "  PyTorch: 6.414803e-04\n",
      "\n",
      "Live token gradients:\n",
      "  Manual:  4.181137e-03\n",
      "  PyTorch: 4.182540e-03\n",
      "\n",
      "Cosine similarity (manual vs pytorch): 1.000495\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"COMPARISON: Manual vs PyTorch\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDead token gradients:\")\n",
    "print(f\"  Manual:  {manual_dead_grad.norm(dim=1).mean():.6e}\")\n",
    "print(f\"  PyTorch: {pytorch_dead_grad.norm(dim=1).mean():.6e}\")\n",
    "\n",
    "print(f\"\\nLive token gradients:\")\n",
    "print(f\"  Manual:  {manual_live_grad.norm(dim=1).mean():.6e}\")\n",
    "print(f\"  PyTorch: {pytorch_live_grad.norm(dim=1).mean():.6e}\")\n",
    "\n",
    "# Cosine similarity between manual and pytorch (both on CPU)\n",
    "dL_dW_manual_cpu = dL_dW_manual.cpu()\n",
    "cos_sim = F.cosine_similarity(dL_dW_manual_cpu.flatten().unsqueeze(0), \n",
    "                               dL_dW_pytorch.flatten().unsqueeze(0)).item()\n",
    "print(f\"\\nCosine similarity (manual vs pytorch): {cos_sim:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive: What's happening with dead token probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dead token probability statistics:\n",
      "  Mean p(dead): 2.508319e-04\n",
      "  Max p(dead):  6.701551e-04\n",
      "  Min p(dead):  9.260746e-05\n",
      "  Sum p(dead) per position: 4.800921e-01\n",
      "\n",
      "Live token probability statistics:\n",
      "  Mean p(live): 2.506789e-04\n",
      "  Max p(live):  6.889911e-04\n",
      "  Min p(live):  8.991521e-05\n",
      "  Sum p(live) per position: 5.199079e-01\n",
      "\n",
      "Uniform would be: 2.507523e-04\n"
     ]
    }
   ],
   "source": [
    "# What probability mass do dead tokens get?\n",
    "dead_probs = probs[:, :, dead_mask_cpu]  # [B, T, N_dead]\n",
    "live_probs = probs[:, :, ~dead_mask_cpu]  # [B, T, N_live]\n",
    "\n",
    "print(f\"Dead token probability statistics:\")\n",
    "print(f\"  Mean p(dead): {dead_probs.mean():.6e}\")\n",
    "print(f\"  Max p(dead):  {dead_probs.max():.6e}\")\n",
    "print(f\"  Min p(dead):  {dead_probs.min():.6e}\")\n",
    "print(f\"  Sum p(dead) per position: {dead_probs.sum(dim=-1).mean():.6e}\")\n",
    "\n",
    "print(f\"\\nLive token probability statistics:\")\n",
    "print(f\"  Mean p(live): {live_probs.mean():.6e}\")\n",
    "print(f\"  Max p(live):  {live_probs.max():.6e}\")\n",
    "print(f\"  Min p(live):  {live_probs.min():.6e}\")\n",
    "print(f\"  Sum p(live) per position: {live_probs.sum(dim=-1).mean():.6e}\")\n",
    "\n",
    "print(f\"\\nUniform would be: {1/vocab_size:.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dead token logit statistics:\n",
      "  Mean: -0.0002\n",
      "  Std:  0.2257\n",
      "  Min:  -0.9727\n",
      "  Max:  1.0078\n",
      "\n",
      "Live token logit statistics:\n",
      "  Mean: -0.0010\n",
      "  Std:  0.2269\n",
      "  Min:  -1.0078\n",
      "  Max:  1.0312\n"
     ]
    }
   ],
   "source": [
    "# What are the logits like?\n",
    "dead_logits = logits[:, :, dead_mask_cpu].float()  # [B, T, N_dead]\n",
    "live_logits = logits[:, :, ~dead_mask_cpu].float()  # [B, T, N_live]\n",
    "\n",
    "print(f\"Dead token logit statistics:\")\n",
    "print(f\"  Mean: {dead_logits.mean():.4f}\")\n",
    "print(f\"  Std:  {dead_logits.std():.4f}\")\n",
    "print(f\"  Min:  {dead_logits.min():.4f}\")\n",
    "print(f\"  Max:  {dead_logits.max():.4f}\")\n",
    "\n",
    "print(f\"\\nLive token logit statistics:\")\n",
    "print(f\"  Mean: {live_logits.mean():.4f}\")\n",
    "print(f\"  Std:  {live_logits.std():.4f}\")\n",
    "print(f\"  Min:  {live_logits.min():.4f}\")\n",
    "print(f\"  Max:  {live_logits.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Culprit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dead token embedding statistics:\n",
      "  Mean norm: 0.225375\n",
      "  Std norm:  0.013891\n",
      "\n",
      "Live token embedding statistics:\n",
      "  Mean norm: 0.226024\n",
      "  Std norm:  0.013994\n"
     ]
    }
   ],
   "source": [
    "# Is the issue in the embedding initialization?\n",
    "W = model.tok_emb.weight.detach().cpu().float()\n",
    "\n",
    "dead_W = W[dead_mask_cpu]\n",
    "live_W = W[~dead_mask_cpu]\n",
    "\n",
    "print(f\"Dead token embedding statistics:\")\n",
    "print(f\"  Mean norm: {dead_W.norm(dim=1).mean():.6f}\")\n",
    "print(f\"  Std norm:  {dead_W.norm(dim=1).std():.6f}\")\n",
    "\n",
    "print(f\"\\nLive token embedding statistics:\")\n",
    "print(f\"  Mean norm: {live_W.norm(dim=1).mean():.6f}\")\n",
    "print(f\"  Std norm:  {live_W.norm(dim=1).std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h · W for dead tokens:\n",
      "  Mean: -0.0025\n",
      "  Std:  0.2284\n",
      "\n",
      "h · W for live tokens:\n",
      "  Mean: -0.0026\n",
      "  Std:  0.2228\n"
     ]
    }
   ],
   "source": [
    "# What's h · W^T giving us?\n",
    "# Take one h vector and compute its dot product with all embeddings\n",
    "h_sample = h_float[0, 0, :].cpu()  # [D] - first position of first batch, move to CPU\n",
    "\n",
    "dots_dead = (dead_W @ h_sample)  # [N_dead]\n",
    "dots_live = (live_W @ h_sample)  # [N_live]\n",
    "\n",
    "print(f\"h · W for dead tokens:\")\n",
    "print(f\"  Mean: {dots_dead.mean():.4f}\")\n",
    "print(f\"  Std:  {dots_dead.std():.4f}\")\n",
    "\n",
    "print(f\"\\nh · W for live tokens:\")\n",
    "print(f\"  Mean: {dots_live.mean():.4f}\")\n",
    "print(f\"  Std:  {dots_live.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"FINDINGS\")\nprint(\"=\"*60)\nprint(\"\"\"\nWith explicit torch.randn * 0.02 initialization:\n\n1. LOGIT GAP IS GONE\n   - Dead logits: mean -0.0002, std 0.23\n   - Live logits: mean -0.0010, std 0.23\n   - Identical distributions!\n\n2. PROBABILITIES ARE UNIFORM\n   - Dead p(token): 2.508e-4\n   - Live p(token): 2.507e-4\n   - Uniform would be: 2.508e-4\n   - No systematic suppression of dead tokens.\n\n3. GRADIENTS ARE DIFFERENT (but correctly so!)\n   - Dead gradient: 6.4e-4\n   - Live gradient: 4.2e-3\n   - Why? Live tokens appear as TARGETS, so they get the (p-1)×h \n     correction term. Dead tokens only get p×h. This is expected.\n\n4. THE ORIGINAL MYSTERY WAS INITIALIZATION\n   - PyTorch's default nn.Embedding uses N(0,1)\n   - Our explicit N(0,0.02) produces ~50× smaller embeddings\n   - With default init, something created 16-unit logit gap\n   - With explicit init, logits are centered at 0 as expected\n\n5. REMAINING QUESTION\n   - Why did PyTorch default create systematic dead/live separation?\n   - Possibly: larger random vectors have more extreme h·W products\n   - The pathology wasn't in the tokenizer—it was in uncontrolled init.\n\"\"\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}