{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Model Test: How Dumb Is Goldilocks?\n",
    "\n",
    "We trained for 10K steps and hit a loss floor at ~6.75. That's perplexity ~850.\n",
    "\n",
    "Let's see what this model actually generates. Is it gibberish, or English-shaped gibberish?\n",
    "\n",
    "---\n",
    "\n",
    "*Jeffery Harrell & Alpha, December 1, 2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_file\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parameters ===\n",
    "GOLDILOCKS_DATA = \"../Goldilocks/data\"\n",
    "TOKENIZER_PATH = f\"{GOLDILOCKS_DATA}/tokenizer.json\"\n",
    "MODEL_PATH = \"model.safetensors\"\n",
    "\n",
    "# Architecture (must match training)\n",
    "N_LAYERS = 4\n",
    "D_MODEL = 128\n",
    "N_HEADS = 2\n",
    "D_FF = 256\n",
    "SEQ_LEN = 128\n",
    "DROPOUT = 0.0\n",
    "MODEL_DTYPE = torch.bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer: 3,988 tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"✓ Tokenizer: {vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model skeleton created\n"
     ]
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, seq_len, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(seq_len, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "                dropout=dropout, activation='gelu', batch_first=True, norm_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.head.weight = self.tok_emb.weight  # Weight tying\n",
    "        self.seq_len = seq_len\n",
    "        self.register_buffer('causal_mask', None)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        if self.causal_mask is None or self.causal_mask.shape[0] != T:\n",
    "            self.causal_mask = torch.triu(\n",
    "                torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1\n",
    "            )\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        h = self.tok_emb(x) + self.pos_emb(pos)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, src_mask=self.causal_mask, is_causal=True)\n",
    "        return self.head(self.ln_f(h))\n",
    "\n",
    "# Create model skeleton\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    d_ff=D_FF,\n",
    "    seq_len=SEQ_LEN,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "print(f\"✓ Model skeleton created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded weights from model.safetensors\n",
      "  State dict keys: 54\n"
     ]
    }
   ],
   "source": [
    "# Load trained weights\n",
    "state_dict = load_file(MODEL_PATH)\n",
    "\n",
    "# Load into model (strict=False because we have weight tying)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to(device).to(MODEL_DTYPE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Loaded weights from {MODEL_PATH}\")\n",
    "print(f\"  State dict keys: {len(state_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generation function ready\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0, top_k=None):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    encoded = tokenizer.encode(prompt)\n",
    "    tokens = torch.tensor(encoded.ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop to seq_len if needed\n",
    "        tokens_cond = tokens[:, -SEQ_LEN:]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(tokens_cond)\n",
    "        logits = logits[:, -1, :] / temperature  # Last position only\n",
    "        \n",
    "        # Optional top-k filtering\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = float('-inf')\n",
    "        \n",
    "        # Sample\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    \n",
    "    # Decode\n",
    "    return tokenizer.decode(tokens[0].tolist())\n",
    "\n",
    "print(\"✓ Generation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Let's See What We Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATION TEST (temperature=1.0, no top-k)\n",
      "============================================================\n",
      "\n",
      "Prompt: 'The '\n",
      "----------------------------------------\n",
      "The Men acc getherch I A r*ag Cor can canitsgIleill Tosace tos ex for,10,e A A lessction sit anday gon short,bTheedightst it X her\n",
      "\n",
      "\n",
      "Prompt: 'Once upon a time'\n",
      "----------------------------------------\n",
      "Once upon a time just timerenting, in of the alsoin besthru\n",
      "ia together Hehe whatandos som f.ort forN att May ser, Not toim teamon? A un ran experience weHibound tr Pil de\n",
      "\n",
      "\n",
      "Prompt: 'In the beginning'\n",
      "----------------------------------------\n",
      "In the beginning Seicn longledge m any con itense restedW.perFuneB following chra obre and just andingal2 countryint too 18's n f sense Aflandn somethingur system ag students sanc f\n",
      "\n",
      "\n",
      "Prompt: 'Hello, my name is'\n",
      "----------------------------------------\n",
      "Hello, my name is hera, Re earicron3cecia,atterra4ayser.\n",
      " two ory W do.Mf placeers.allet Uteram new U're re.00ust RMve m line theEBcesTly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try a few prompts\n",
    "prompts = [\n",
    "    \"The \",\n",
    "    \"Once upon a time\",\n",
    "    \"In the beginning\",\n",
    "    \"Hello, my name is\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATION TEST (temperature=1.0, no top-k)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {repr(prompt)}\")\n",
    "    print(\"-\" * 40)\n",
    "    output = generate(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0)\n",
    "    print(output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATION TEST (temperature=0.5, top_k=10)\n",
      "============================================================\n",
      "\n",
      "Prompt: 'The '\n",
      "----------------------------------------\n",
      "The  ofs a to the to thes of a a the a, to.s and a the aings, the of. and, and a thes and,s a and in, the the of as the in. the and\n",
      "\n",
      "\n",
      "Prompt: 'Once upon a time'\n",
      "----------------------------------------\n",
      "Once upon a time. the the, ands the and to a. to a the thes the a, to the the, to a of, the the and thes the of. a and theing and the the the,, the the theings\n",
      "\n",
      "\n",
      "Prompt: 'In the beginning'\n",
      "----------------------------------------\n",
      "In the beginning and thes to the the the,s to a and the. the the a the in to. the. of, to,. and of the, of a the the a, a a to to and of and, the., the\n",
      "\n",
      "\n",
      "Prompt: 'Hello, my name is'\n",
      "----------------------------------------\n",
      "Hello, my name is to of to,s.. the. to,. and to. tos to thes theed, the, ands the and, a the and.s the,, theing a the.s the and the, and.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try with lower temperature (more deterministic)\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATION TEST (temperature=0.5, top_k=10)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {repr(prompt)}\")\n",
    "    print(\"-\" * 40)\n",
    "    output = generate(model, tokenizer, prompt, max_new_tokens=50, temperature=0.5, top_k=10)\n",
    "    print(output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATION TEST (greedy, temperature=0.1)\n",
      "============================================================\n",
      "\n",
      "Prompt: 'The '\n",
      "----------------------------------------\n",
      "The  the the the the the the the the the the the the the and the the the the the the the the the the the the the the the the the the the the the and the the the the the, the the the the the the the the\n",
      "\n",
      "\n",
      "Prompt: 'Once upon a time'\n",
      "----------------------------------------\n",
      "Once upon a time the the the the the the the the the, the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "\n",
      "Prompt: 'In the beginning'\n",
      "----------------------------------------\n",
      "In the beginning the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the, the the the the the the the the\n",
      "\n",
      "\n",
      "Prompt: 'Hello, my name is'\n",
      "----------------------------------------\n",
      "Hello, my name is the the the the the the the the the the the the the the the the the the the the the the the the, the the the the the the the the the the the the the the the the the the the the the. the the the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Greedy decoding (temperature → 0)\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATION TEST (greedy, temperature=0.1)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {repr(prompt)}\")\n",
    "    print(\"-\" * 40)\n",
    "    output = generate(model, tokenizer, prompt, max_new_tokens=50, temperature=0.1)\n",
    "    print(output)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
