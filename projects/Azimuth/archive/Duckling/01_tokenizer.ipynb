{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Build the Duckling Tokenizer\n",
    "\n",
    "Truncate GPT-2 tokenizer to 8,000 tokens. These are our \"live\" tokens.\n",
    "\n",
    "The embedding matrix will be padded to 10,000 rows during training,\n",
    "creating 2,000 \"phantom\" dead tokens that can never appear in data.\n",
    "\n",
    "---\n",
    "\n",
    "*Jeffery Harrell & Alpha, December 1, 2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T23:01:56.814808Z",
     "iopub.status.busy": "2025-12-01T23:01:56.814652Z",
     "iopub.status.idle": "2025-12-01T23:01:58.465189Z",
     "shell.execute_reply": "2025-12-01T23:01:58.464776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target vocab: 8000\n",
      "Base vocab (bytes): 256\n",
      "Target merges: 7742\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# How many merge rules to keep (vocab = BASE_VOCAB + TARGET_MERGES + specials)\n",
    "# GPT-2 has 256 byte tokens + 50,000 merges = 50,257 vocab\n",
    "# We want ~8,000 total, so: 8000 - 256 - 2 (eos, pad) = 7,742 merges\n",
    "TARGET_VOCAB = 8_000\n",
    "BASE_VOCAB = 256  # Byte-level tokens\n",
    "TARGET_MERGES = TARGET_VOCAB - BASE_VOCAB - 2  # Leave room for eos + pad\n",
    "\n",
    "OUTPUT_DIR = Path(\"data/tokenizer\")\n",
    "\n",
    "print(f\"Target vocab: {TARGET_VOCAB}\")\n",
    "print(f\"Base vocab (bytes): {BASE_VOCAB}\")\n",
    "print(f\"Target merges: {TARGET_MERGES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T23:01:58.478882Z",
     "iopub.status.busy": "2025-12-01T23:01:58.478715Z",
     "iopub.status.idle": "2025-12-01T23:01:59.762825Z",
     "shell.execute_reply": "2025-12-01T23:01:59.762384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GPT-2 vocab size: 50258\n",
      "EOS token: '<|endoftext|>' (id=50256)\n",
      "PAD token: '<|pad|>' (id=50257)\n"
     ]
    }
   ],
   "source": [
    "# Load the full GPT-2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "\n",
    "print(f\"Original GPT-2 vocab size: {len(tokenizer)}\")\n",
    "print(f\"EOS token: {repr(tokenizer.eos_token)} (id={tokenizer.eos_token_id})\")\n",
    "print(f\"PAD token: {repr(tokenizer.pad_token)} (id={tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T23:01:59.764014Z",
     "iopub.status.busy": "2025-12-01T23:01:59.763904Z",
     "iopub.status.idle": "2025-12-01T23:01:59.797013Z",
     "shell.execute_reply": "2025-12-01T23:01:59.796639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full vocab entries: 50257\n",
      "Full merge rules: 50000\n",
      "Header: #version: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Save full tokenizer to temp dir so we can extract vocab.json and merges.txt\n",
    "tmp_dir = OUTPUT_DIR / \"tmp-full\"\n",
    "tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "tokenizer.save_pretrained(tmp_dir)\n",
    "\n",
    "# Load vocab and merges\n",
    "with open(tmp_dir / \"vocab.json\") as f:\n",
    "    full_vocab = json.load(f)\n",
    "\n",
    "with open(tmp_dir / \"merges.txt\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "header, merge_rules = lines[0], lines[1:]\n",
    "\n",
    "print(f\"Full vocab entries: {len(full_vocab)}\")\n",
    "print(f\"Full merge rules: {len(merge_rules)}\")\n",
    "print(f\"Header: {header}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T23:01:59.798168Z",
     "iopub.status.busy": "2025-12-01T23:01:59.798106Z",
     "iopub.status.idle": "2025-12-01T23:01:59.801756Z",
     "shell.execute_reply": "2025-12-01T23:01:59.801364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated merges: 7742\n",
      "New vocab size: 8000\n"
     ]
    }
   ],
   "source": [
    "# Truncate merge rules\n",
    "truncated_merge_rules = merge_rules[:TARGET_MERGES]\n",
    "\n",
    "# Build new vocab: first BASE_VOCAB bytes + TARGET_MERGES merged tokens\n",
    "inverse_vocab = {idx: token for token, idx in full_vocab.items()}\n",
    "keep_ids = list(range(BASE_VOCAB + TARGET_MERGES))\n",
    "kept_tokens = [inverse_vocab[i] for i in keep_ids]\n",
    "\n",
    "# Make sure special tokens are included\n",
    "for token in [tokenizer.eos_token, tokenizer.pad_token]:\n",
    "    if token not in kept_tokens:\n",
    "        kept_tokens.append(token)\n",
    "\n",
    "# Create new vocab mapping\n",
    "new_vocab = {token: i for i, token in enumerate(kept_tokens)}\n",
    "\n",
    "print(f\"Truncated merges: {len(truncated_merge_rules)}\")\n",
    "print(f\"New vocab size: {len(new_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Truncated Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T23:01:59.802760Z",
     "iopub.status.busy": "2025-12-01T23:01:59.802705Z",
     "iopub.status.idle": "2025-12-01T23:01:59.807917Z",
     "shell.execute_reply": "2025-12-01T23:01:59.807580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved vocab.json (8000 entries)\n",
      "Saved merges.txt (7742 rules)\n"
     ]
    }
   ],
   "source": [
    "# Save new vocab and merges\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "with open(OUTPUT_DIR / \"vocab.json\", \"w\") as f:\n",
    "    json.dump(new_vocab, f, ensure_ascii=False)\n",
    "\n",
    "with open(OUTPUT_DIR / \"merges.txt\", \"w\") as f:\n",
    "    f.write(header + \"\\n\")\n",
    "    for merge in truncated_merge_rules:\n",
    "        f.write(merge + \"\\n\")\n",
    "\n",
    "print(f\"Saved vocab.json ({len(new_vocab)} entries)\")\n",
    "print(f\"Saved merges.txt ({len(truncated_merge_rules)} rules)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T23:01:59.808847Z",
     "iopub.status.busy": "2025-12-01T23:01:59.808773Z",
     "iopub.status.idle": "2025-12-01T23:01:59.820447Z",
     "shell.execute_reply": "2025-12-01T23:01:59.820083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TOKENIZER SAVED\n",
      "==================================================\n",
      "Vocab size: 8000\n",
      "Location: /Users/jefferyharrell/Workshop/projects/Azimuth/Duckling/data/tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Load as a proper tokenizer and save with all config files\n",
    "new_tokenizer = GPT2TokenizerFast(\n",
    "    vocab_file=str(OUTPUT_DIR / \"vocab.json\"),\n",
    "    merges_file=str(OUTPUT_DIR / \"merges.txt\"),\n",
    ")\n",
    "new_tokenizer.pad_token = tokenizer.pad_token\n",
    "new_tokenizer.eos_token = tokenizer.eos_token\n",
    "new_tokenizer.bos_token = tokenizer.eos_token  # GPT-2 uses eos as bos\n",
    "\n",
    "new_tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TOKENIZER SAVED\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Vocab size: {len(new_tokenizer)}\")\n",
    "print(f\"Location: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T23:01:59.821512Z",
     "iopub.status.busy": "2025-12-01T23:01:59.821457Z",
     "iopub.status.idle": "2025-12-01T23:01:59.823507Z",
     "shell.execute_reply": "2025-12-01T23:01:59.823114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up data/tokenizer/tmp-full\n"
     ]
    }
   ],
   "source": [
    "# Remove temp directory\n",
    "import shutil\n",
    "shutil.rmtree(tmp_dir)\n",
    "print(f\"Cleaned up {tmp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T23:01:59.824407Z",
     "iopub.status.busy": "2025-12-01T23:01:59.824347Z",
     "iopub.status.idle": "2025-12-01T23:01:59.836631Z",
     "shell.execute_reply": "2025-12-01T23:01:59.836265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text: 'Once upon a time there was a little girl named Lily.'\n",
      "Token IDs: [7454, 2402, 257, 640, 612, 373, 257, 1310, 2576, 3706, 406, 813, 13]\n",
      "Decoded: 'Once upon a time there was a little girl named Lily.'\n",
      "Num tokens: 13\n",
      "\n",
      "Max token ID in encoding: 7454\n",
      "Vocab size: 8000\n",
      "\n",
      "✓ All token IDs within vocab range\n"
     ]
    }
   ],
   "source": [
    "# Reload and test\n",
    "test_tokenizer = GPT2TokenizerFast.from_pretrained(OUTPUT_DIR)\n",
    "\n",
    "test_text = \"Once upon a time there was a little girl named Lily.\"\n",
    "tokens = test_tokenizer.encode(test_text)\n",
    "\n",
    "print(f\"Test text: {repr(test_text)}\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Decoded: {repr(test_tokenizer.decode(tokens))}\")\n",
    "print(f\"Num tokens: {len(tokens)}\")\n",
    "print(f\"\\nMax token ID in encoding: {max(tokens)}\")\n",
    "print(f\"Vocab size: {len(test_tokenizer)}\")\n",
    "\n",
    "# Verify all tokens are in range\n",
    "assert max(tokens) < len(test_tokenizer), \"Token ID out of range!\"\n",
    "print(\"\\n✓ All token IDs within vocab range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T23:01:59.837613Z",
     "iopub.status.busy": "2025-12-01T23:01:59.837542Z",
     "iopub.status.idle": "2025-12-01T23:01:59.840087Z",
     "shell.execute_reply": "2025-12-01T23:01:59.839819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DUCKLING TOKENIZER READY\n",
      "==================================================\n",
      "Live tokens: 8,000\n",
      "Phantom tokens (added during training): 2,000\n",
      "Total embedding rows: 10,000\n",
      "Dead token ratio: 20%\n",
      "\n",
      "Location: /Users/jefferyharrell/Workshop/projects/Azimuth/Duckling/data/tokenizer\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"DUCKLING TOKENIZER READY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Live tokens: {len(test_tokenizer):,}\")\n",
    "print(f\"Phantom tokens (added during training): 2,000\")\n",
    "print(f\"Total embedding rows: 10,000\")\n",
    "print(f\"Dead token ratio: 20%\")\n",
    "print(f\"\\nLocation: {OUTPUT_DIR.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
