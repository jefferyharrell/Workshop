{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# The Math of Attention\n",
    "\n",
    "*Gently, for an old man.*\n",
    "\n",
    "---\n",
    "\n",
    "We're going to build up the attention mechanism piece by piece. Each section will have:\n",
    "1. The intuition (what are we doing and why)\n",
    "2. The equation (the actual math, in LaTeX)\n",
    "3. The code (runnable PyTorch, explicit operations)\n",
    "\n",
    "By the end, you'll have a working attention implementation you built yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to learn.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# We'll work in float32 on CPU for clarity\n",
    "# (No need for GPU at this scale)\n",
    "\n",
    "print(\"Ready to learn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Starting Point: Embedded Tokens\n",
    "\n",
    "We begin with a sequence of tokens that have already been embedded. Each token is a vector of dimension $d$.\n",
    "\n",
    "If we have $n$ tokens, our input is a matrix $X$ of shape $(n, d)$:\n",
    "\n",
    "$$X \\in \\mathbb{R}^{n \\times d}$$\n",
    "\n",
    "Each row is one token's embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "section-1-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X shape: torch.Size([4, 8])\n",
      "\n",
      "X = \n",
      "[[ 1.93  1.49  0.9  -2.11  0.68 -1.23 -0.04 -1.6 ]\n",
      " [-0.75  1.65 -0.39 -1.4  -0.73 -0.56 -0.77  0.76]\n",
      " [ 1.64 -0.16 -0.5   0.44 -0.76  1.08  0.8   1.68]\n",
      " [ 1.28  1.3   0.61  1.33 -0.23  0.04 -0.25  0.86]]\n"
     ]
    }
   ],
   "source": [
    "# Let's make this concrete.\n",
    "# Tiny example: 4 tokens, 8-dimensional embeddings\n",
    "\n",
    "n_tokens = 4\n",
    "d_model = 8\n",
    "\n",
    "# Random embeddings (pretend these came from an embedding layer)\n",
    "X = torch.randn(n_tokens, d_model)\n",
    "\n",
    "print(f\"Input X shape: {X.shape}\")\n",
    "print(f\"\\nX = \")\n",
    "print(X.numpy().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Three Projections: Q, K, V\n",
    "\n",
    "Each token plays three roles:\n",
    "- **Query (Q):** \"What am I looking for?\"\n",
    "- **Key (K):** \"What do I advertise?\"\n",
    "- **Value (V):** \"What do I offer if selected?\"\n",
    "\n",
    "We create these by multiplying $X$ by three learned weight matrices:\n",
    "\n",
    "$$Q = X W_Q$$\n",
    "$$K = X W_K$$\n",
    "$$V = X W_V$$\n",
    "\n",
    "Where:\n",
    "- $W_Q, W_K \\in \\mathbb{R}^{d \\times d_k}$ (project to key/query dimension)\n",
    "- $W_V \\in \\mathbb{R}^{d \\times d_v}$ (project to value dimension)\n",
    "\n",
    "Often $d_k = d_v = d$ for simplicity, but they don't have to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "section-2-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([4, 8])\n",
      "K shape: torch.Size([4, 8])\n",
      "V shape: torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# For our tiny example, let's keep all dimensions the same\n",
    "d_k = d_model  # query and key dimension\n",
    "d_v = d_model  # value dimension\n",
    "\n",
    "# Initialize random projection matrices\n",
    "# (In a real model, these are learned parameters)\n",
    "W_Q = torch.randn(d_model, d_k)\n",
    "W_K = torch.randn(d_model, d_k)\n",
    "W_V = torch.randn(d_model, d_v)\n",
    "\n",
    "# Project!\n",
    "Q = X @ W_Q  # (n, d) @ (d, d_k) = (n, d_k)\n",
    "K = X @ W_K  # (n, d) @ (d, d_k) = (n, d_k)\n",
    "V = X @ W_V  # (n, d) @ (d, d_v) = (n, d_v)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Attention Scores: Queries Meet Keys\n",
    "\n",
    "Now we compute how much each query \"matches\" each key. This is a dot product between every query-key pair.\n",
    "\n",
    "$$\\text{scores} = Q K^T$$\n",
    "\n",
    "Result shape: $(n, n)$. Entry $(i, j)$ tells us how much token $i$'s query matches token $j$'s key.\n",
    "\n",
    "**This is the N² part.** Every token compared to every other token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "section-3-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores shape: torch.Size([4, 4])\n",
      "\n",
      "Scores = \n",
      "[[ 48.36  -1.43   7.06  16.17]\n",
      " [  1.88  14.59 -10.85 -11.88]\n",
      " [-20.9   -3.98  16.85   5.96]\n",
      " [  7.22   3.67  49.61  35.63]]\n"
     ]
    }
   ],
   "source": [
    "# Compute attention scores\n",
    "scores = Q @ K.T  # (n, d_k) @ (d_k, n) = (n, n)\n",
    "\n",
    "print(f\"Scores shape: {scores.shape}\")\n",
    "print(f\"\\nScores = \")\n",
    "print(scores.numpy().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Scaling: Preventing Exploding Dot Products\n",
    "\n",
    "Here's a subtle but important detail. When $d_k$ is large, dot products tend to be large (they're sums of $d_k$ terms). Large inputs to softmax push it into regions where gradients vanish.\n",
    "\n",
    "**Solution:** Scale down by $\\sqrt{d_k}$.\n",
    "\n",
    "$$\\text{scaled\\_scores} = \\frac{Q K^T}{\\sqrt{d_k}}$$\n",
    "\n",
    "This keeps the variance of the scores roughly constant regardless of dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "section-4-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale factor: 2.83\n",
      "\n",
      "Scaled scores = \n",
      "[[17.1  -0.51  2.5   5.72]\n",
      " [ 0.67  5.16 -3.84 -4.2 ]\n",
      " [-7.39 -1.41  5.96  2.11]\n",
      " [ 2.55  1.3  17.54 12.6 ]]\n"
     ]
    }
   ],
   "source": [
    "# Scale the scores\n",
    "scale = d_k ** 0.5\n",
    "scaled_scores = scores / scale\n",
    "\n",
    "print(f\"Scale factor: {scale:.2f}\")\n",
    "print(f\"\\nScaled scores = \")\n",
    "print(scaled_scores.numpy().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Softmax: Turning Scores into Weights\n",
    "\n",
    "We need each token's attention to be a probability distribution over all positions—weights that sum to 1.\n",
    "\n",
    "Softmax does this, applied **row-wise** (each token's scores become a distribution):\n",
    "\n",
    "$$\\text{attention}_{ij} = \\frac{\\exp(\\text{scaled\\_scores}_{ij})}{\\sum_k \\exp(\\text{scaled\\_scores}_{ik})}$$\n",
    "\n",
    "Or more compactly:\n",
    "\n",
    "$$\\text{attention} = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "Each row of the resulting matrix sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "section-5-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights shape: torch.Size([4, 4])\n",
      "\n",
      "Attention weights = \n",
      "[[1.    0.    0.    0.   ]\n",
      " [0.011 0.989 0.    0.   ]\n",
      " [0.    0.001 0.979 0.021]\n",
      " [0.    0.    0.993 0.007]]\n",
      "\n",
      "Row sums: [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax row-wise (dim=-1 means last dimension, i.e., across columns)\n",
    "attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention weights = \")\n",
    "print(attention_weights.numpy().round(3))\n",
    "\n",
    "# Verify rows sum to 1\n",
    "print(f\"\\nRow sums: {attention_weights.sum(dim=-1).numpy().round(6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. The Output: Weighted Sum of Values\n",
    "\n",
    "Finally, each token gathers information by taking a weighted sum of all the value vectors, using the attention weights.\n",
    "\n",
    "$$\\text{output} = \\text{attention} \\cdot V$$\n",
    "\n",
    "Shape: $(n, n) \\cdot (n, d_v) = (n, d_v)$\n",
    "\n",
    "Token $i$'s output is: $\\sum_j \\text{attention}_{ij} \\cdot V_j$\n",
    "\n",
    "A blend of all values, weighted by how much token $i$ attended to each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "section-6-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 8])\n",
      "\n",
      "Output = \n",
      "[[-3.69  0.8   9.47 -2.52 -6.27 -0.84 -3.96 -3.32]\n",
      " [-1.78  5.17  3.8   2.56 -3.    1.6   0.38  5.11]\n",
      " [-5.22  3.38 -5.24  0.9   3.28 -0.42  3.67 -0.99]\n",
      " [-5.21  3.4  -5.28  0.9   3.34 -0.39  3.69 -1.06]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the output\n",
    "output = attention_weights @ V  # (n, n) @ (n, d_v) = (n, d_v)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nOutput = \")\n",
    "print(output.numpy().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. The Full Equation\n",
    "\n",
    "Putting it all together, scaled dot-product attention is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "That's it. That's the whole thing.\n",
    "\n",
    "Let's wrap it in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "section-7-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs match: True\n",
      "Weights match: True\n"
     ]
    }
   ],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (n, d_k)\n",
    "        K: Keys, shape (n, d_k)\n",
    "        V: Values, shape (n, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: shape (n, d_v)\n",
    "        attention_weights: shape (n, n)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute scaled scores\n",
    "    scores = Q @ K.T / (d_k ** 0.5)\n",
    "    \n",
    "    # Softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# Verify it matches what we computed step-by-step\n",
    "output_check, weights_check = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Outputs match: {torch.allclose(output, output_check)}\")\n",
    "print(f\"Weights match: {torch.allclose(attention_weights, weights_check)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "We've built **single-head attention**. To get to a full transformer, we still need:\n",
    "\n",
    "1. **Causal masking** — so tokens can't attend to future positions\n",
    "2. **Multi-head attention** — running several attention heads in parallel\n",
    "3. **Feed-forward layer** — the other half of a transformer block\n",
    "4. **Layer normalization** — keeping activations well-behaved\n",
    "5. **Positional encoding** — since attention is position-agnostic\n",
    "6. **The full transformer block** — putting it all together\n",
    "\n",
    "One cell at a time. One concept at a time.\n",
    "\n",
    "---\n",
    "\n",
    "*\"See one, do one, teach one.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
