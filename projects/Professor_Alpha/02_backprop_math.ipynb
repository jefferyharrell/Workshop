{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# The Math of Backpropagation\n",
    "\n",
    "*Following the gradient home.*\n",
    "\n",
    "---\n",
    "\n",
    "We're going to trace backpropagation through our bag-of-words model from notebook 01. The model is simple enough that we can compute every gradient by hand, then verify with PyTorch.\n",
    "\n",
    "**The model:**\n",
    "```\n",
    "tokens → Embedding → Average → Linear → Softmax → Loss\n",
    "```\n",
    "\n",
    "**Our goal:** Compute ∂loss/∂everything. By the end, you'll understand:\n",
    "1. What the chain rule actually does\n",
    "2. How gradients flow backward through each operation\n",
    "3. Why this is called \"backpropagation\"\n",
    "4. What `.backward()` is actually computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to learn.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# We'll work with tiny dimensions so we can see everything\n",
    "print(\"Ready to learn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-0-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. The Big Picture: What Is Backpropagation?\n",
    "\n",
    "Training a neural network means adjusting weights to minimize loss. To know *how* to adjust them, we need gradients—the derivative of the loss with respect to each weight.\n",
    "\n",
    "**The problem:** Our network is a composition of functions:\n",
    "$$\\text{loss} = f_5(f_4(f_3(f_2(f_1(\\text{input})))))$$\n",
    "\n",
    "How do we compute $\\frac{\\partial \\text{loss}}{\\partial W}$ for some weight $W$ buried deep in $f_2$?\n",
    "\n",
    "**The answer:** The chain rule.\n",
    "\n",
    "### The Chain Rule (Single Variable)\n",
    "\n",
    "If $y = f(g(x))$, then:\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "You multiply the derivatives along the path.\n",
    "\n",
    "### The Chain Rule (Many Variables)\n",
    "\n",
    "If $L$ depends on $y$, and $y$ depends on $x$, then:\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "With vectors/matrices, these become Jacobians, but the principle is the same: **gradients flow backward through the computation graph, multiplying at each step.**\n",
    "\n",
    "### Why \"Back\" Propagation?\n",
    "\n",
    "Because we start at the end (the loss) and work backward. At each step, we ask: \"Given how the loss changes with respect to this layer's *output*, how does it change with respect to this layer's *input* and *parameters*?\"\n",
    "\n",
    "That's the whole algorithm. Let's see it in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Our Model: A Tiny Bag of Words\n",
    "\n",
    "Let's build the smallest possible version of our amoeba:\n",
    "- Vocabulary: 4 tokens\n",
    "- Embedding dimension: 3\n",
    "- Input sequence: 2 tokens\n",
    "\n",
    "Small enough to trace every number by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tiny-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_embed shape: torch.Size([4, 3])\n",
      "W_unembed shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# === Tiny dimensions ===\n",
    "vocab_size = 4\n",
    "d_model = 3\n",
    "seq_len = 2\n",
    "\n",
    "# === Initialize weights manually ===\n",
    "# (So we know exactly what they are)\n",
    "\n",
    "# Embedding matrix: vocab_size × d_model\n",
    "# Each row is one token's embedding\n",
    "W_embed = torch.tensor([\n",
    "    [1.0, 0.0, 0.0],   # Token 0\n",
    "    [0.0, 1.0, 0.0],   # Token 1  \n",
    "    [0.0, 0.0, 1.0],   # Token 2\n",
    "    [1.0, 1.0, 1.0],   # Token 3\n",
    "], requires_grad=True)\n",
    "\n",
    "# Unembed matrix: d_model × vocab_size (we'll use W @ x, so it's transposed from nn.Linear)\n",
    "# For simplicity, let's make it the transpose of embedding (no bias)\n",
    "W_unembed = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.5],\n",
    "    [0.0, 1.0, 0.0, 0.5],\n",
    "    [0.0, 0.0, 1.0, 0.5],\n",
    "], requires_grad=True)\n",
    "\n",
    "print(f\"W_embed shape: {W_embed.shape}\")\n",
    "print(f\"W_unembed shape: {W_unembed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "input-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: [1, 2]\n",
      "Target: 3\n"
     ]
    }
   ],
   "source": [
    "# === Input ===\n",
    "# A sequence of 2 tokens: [token 1, token 2]\n",
    "# Target: token 3\n",
    "\n",
    "input_tokens = torch.tensor([1, 2])  # Token IDs\n",
    "target = torch.tensor(3)             # What we want to predict\n",
    "\n",
    "print(f\"Input tokens: {input_tokens.tolist()}\")\n",
    "print(f\"Target: {target.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Forward Pass: Computing the Loss\n",
    "\n",
    "Let's trace the forward pass step by step, naming every intermediate value.\n",
    "\n",
    "### Step 1: Embedding Lookup\n",
    "$$e_i = W_{\\text{embed}}[\\text{token}_i]$$\n",
    "\n",
    "This is just indexing—grab the rows corresponding to our input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forward-embed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Embedding lookup\n",
      "  Token 1 → embedding: [0.0, 1.0, 0.0]\n",
      "  Token 2 → embedding: [0.0, 0.0, 1.0]\n",
      "  e shape: torch.Size([2, 3])\n",
      "  e = \n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Embedding lookup\n",
    "# For each token ID, grab that row from the embedding matrix\n",
    "\n",
    "e = W_embed[input_tokens]  # Shape: [seq_len, d_model] = [2, 3]\n",
    "\n",
    "print(\"Step 1: Embedding lookup\")\n",
    "print(f\"  Token 1 → embedding: {e[0].tolist()}\")\n",
    "print(f\"  Token 2 → embedding: {e[1].tolist()}\")\n",
    "print(f\"  e shape: {e.shape}\")\n",
    "print(f\"  e = \")\n",
    "print(e.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-avg-intro",
   "metadata": {},
   "source": [
    "### Step 2: Average Pooling\n",
    "$$h = \\frac{1}{n} \\sum_{i=1}^{n} e_i$$\n",
    "\n",
    "We collapse the sequence into a single vector by averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "forward-avg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Average pooling\n",
      "  h = mean([[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n",
      "  h = [0.0, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Average the embeddings\n",
    "h = e.mean(dim=0)  # Shape: [d_model] = [3]\n",
    "\n",
    "print(\"Step 2: Average pooling\")\n",
    "print(f\"  h = mean([{e[0].tolist()}, {e[1].tolist()}])\")\n",
    "print(f\"  h = {h.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-linear-intro",
   "metadata": {},
   "source": [
    "### Step 3: Linear Projection (Unembed)\n",
    "$$z = W_{\\text{unembed}}^T h$$\n",
    "\n",
    "Project the hidden state to vocabulary-sized logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forward-linear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Linear projection\n",
      "  z = h @ W_unembed\n",
      "  z = [0.0, 0.5, 0.5, 0.5]\n",
      "  (These are 'logits' — unnormalized scores for each token)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Linear projection to logits\n",
    "# z = W_unembed.T @ h (or equivalently, h @ W_unembed)\n",
    "\n",
    "z = h @ W_unembed  # Shape: [vocab_size] = [4]\n",
    "\n",
    "print(\"Step 3: Linear projection\")\n",
    "print(f\"  z = h @ W_unembed\")\n",
    "print(f\"  z = {z.tolist()}\")\n",
    "print(f\"  (These are 'logits' — unnormalized scores for each token)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-softmax-intro",
   "metadata": {},
   "source": [
    "### Step 4: Softmax\n",
    "$$p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "\n",
    "Turn logits into a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "forward-softmax",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Softmax\n",
      "  p = softmax(z)\n",
      "  p = ['0.1682', '0.2773', '0.2773', '0.2773']\n",
      "  Sum: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Softmax\n",
    "p = F.softmax(z, dim=0)  # Shape: [vocab_size] = [4]\n",
    "\n",
    "print(\"Step 4: Softmax\")\n",
    "print(f\"  p = softmax(z)\")\n",
    "print(f\"  p = {[f'{x:.4f}' for x in p.tolist()]}\")\n",
    "print(f\"  Sum: {p.sum().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-loss-intro",
   "metadata": {},
   "source": [
    "### Step 5: Cross-Entropy Loss\n",
    "$$L = -\\log(p_{\\text{target}})$$\n",
    "\n",
    "How surprised are we by the correct answer? Lower probability → higher loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forward-loss",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Cross-entropy loss\n",
      "  Target: token 3\n",
      "  p[target] = p[3] = 0.2773\n",
      "  L = -log(0.2773) = 1.2827\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Cross-entropy loss\n",
    "# L = -log(p[target])\n",
    "\n",
    "L = -torch.log(p[target])\n",
    "\n",
    "print(\"Step 5: Cross-entropy loss\")\n",
    "print(f\"  Target: token {target.item()}\")\n",
    "print(f\"  p[target] = p[{target.item()}] = {p[target].item():.4f}\")\n",
    "print(f\"  L = -log({p[target].item():.4f}) = {L.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "forward-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FORWARD PASS COMPLETE\n",
      "==================================================\n",
      "\n",
      "Input tokens: [1, 2]\n",
      "Target: 3\n",
      "\n",
      "e (embeddings):     shape torch.Size([2, 3])\n",
      "h (averaged):       [0.0, 0.5, 0.5]\n",
      "z (logits):         ['0.000', '0.500', '0.500', '0.500']\n",
      "p (probabilities):  ['0.168', '0.277', '0.277', '0.277']\n",
      "L (loss):           1.2827\n"
     ]
    }
   ],
   "source": [
    "# Summary of forward pass\n",
    "print(\"=\" * 50)\n",
    "print(\"FORWARD PASS COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nInput tokens: {input_tokens.tolist()}\")\n",
    "print(f\"Target: {target.item()}\")\n",
    "print(f\"\")\n",
    "print(f\"e (embeddings):     shape {e.shape}\")\n",
    "print(f\"h (averaged):       {h.tolist()}\")\n",
    "print(f\"z (logits):         {[f'{x:.3f}' for x in z.tolist()]}\")\n",
    "print(f\"p (probabilities):  {[f'{x:.3f}' for x in p.tolist()]}\")\n",
    "print(f\"L (loss):           {L.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Backward Pass: The Chain Rule in Action\n",
    "\n",
    "Now we go backward. At each step, we'll:\n",
    "1. State the relationship (what depends on what)\n",
    "2. Derive the gradient formula\n",
    "3. Compute it numerically\n",
    "4. Verify with PyTorch\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "At each layer, we receive $\\frac{\\partial L}{\\partial \\text{output}}$ from the layer above, and we compute:\n",
    "- $\\frac{\\partial L}{\\partial \\text{input}}$ — to pass to the layer below\n",
    "- $\\frac{\\partial L}{\\partial W}$ — to update this layer's weights\n",
    "\n",
    "This is why it's called back*propagation*—the gradient propagates backward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backward-loss-intro",
   "metadata": {},
   "source": [
    "### Step 5 (backward): ∂L/∂p\n",
    "\n",
    "We have $L = -\\log(p_{\\text{target}})$.\n",
    "\n",
    "The derivative of $-\\log(x)$ is $-1/x$, so:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial p_i} = \\begin{cases} -1/p_i & \\text{if } i = \\text{target} \\\\ 0 & \\text{otherwise} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "backward-loss",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 backward: ∂L/∂p\n",
      "  ∂L/∂p = ['0.0000', '0.0000', '0.0000', '-3.6065']\n",
      "  (Only position 3 is non-zero)\n"
     ]
    }
   ],
   "source": [
    "# Step 5 backward: ∂L/∂p\n",
    "\n",
    "dL_dp = torch.zeros(vocab_size)\n",
    "dL_dp[target] = -1.0 / p[target].item()\n",
    "\n",
    "print(\"Step 5 backward: ∂L/∂p\")\n",
    "print(f\"  ∂L/∂p = {[f'{x:.4f}' for x in dL_dp.tolist()]}\")\n",
    "print(f\"  (Only position {target.item()} is non-zero)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backward-softmax-intro",
   "metadata": {},
   "source": [
    "### Step 4 (backward): ∂L/∂z (through softmax)\n",
    "\n",
    "This is the trickiest part. Softmax has a coupling: every $z_j$ affects every $p_i$.\n",
    "\n",
    "The Jacobian of softmax is:\n",
    "$$\\frac{\\partial p_i}{\\partial z_j} = p_i(\\delta_{ij} - p_j)$$\n",
    "\n",
    "Where $\\delta_{ij}$ is 1 if $i=j$, else 0.\n",
    "\n",
    "Using the chain rule:\n",
    "$$\\frac{\\partial L}{\\partial z_j} = \\sum_i \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_j}$$\n",
    "\n",
    "**But here's a beautiful simplification.** For cross-entropy loss with softmax:\n",
    "$$\\frac{\\partial L}{\\partial z_i} = p_i - y_i$$\n",
    "\n",
    "Where $y$ is the one-hot target vector. The prediction minus the truth. Elegant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "backward-softmax",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 backward: ∂L/∂z\n",
      "  p =     ['0.1682', '0.2773', '0.2773', '0.2773']\n",
      "  y =     ['0.0000', '0.0000', '0.0000', '1.0000']\n",
      "  ∂L/∂z = ['0.1682', '0.2773', '0.2773', '-0.7227']\n",
      "\n",
      "  Notice: for the target token (3), gradient is p-1 = 0.2773 - 1 = -0.7227\n",
      "  For other tokens, gradient equals their probability (pushing them down)\n"
     ]
    }
   ],
   "source": [
    "# Step 4 backward: ∂L/∂z\n",
    "# For cross-entropy + softmax: ∂L/∂z = p - y (one-hot)\n",
    "\n",
    "# Create one-hot target\n",
    "y_onehot = torch.zeros(vocab_size)\n",
    "y_onehot[target] = 1.0\n",
    "\n",
    "# The gradient is just p - y\n",
    "dL_dz = p.detach() - y_onehot\n",
    "\n",
    "print(\"Step 4 backward: ∂L/∂z\")\n",
    "print(f\"  p =     {[f'{x:.4f}' for x in p.detach().tolist()]}\")\n",
    "print(f\"  y =     {[f'{x:.4f}' for x in y_onehot.tolist()]}\")\n",
    "print(f\"  ∂L/∂z = {[f'{x:.4f}' for x in dL_dz.tolist()]}\")\n",
    "print(f\"\")\n",
    "print(f\"  Notice: for the target token ({target.item()}), gradient is p-1 = {p[target].item():.4f} - 1 = {dL_dz[target].item():.4f}\")\n",
    "print(f\"  For other tokens, gradient equals their probability (pushing them down)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backward-linear-intro",
   "metadata": {},
   "source": [
    "### Step 3 (backward): ∂L/∂h and ∂L/∂W_unembed\n",
    "\n",
    "We had $z = h \\cdot W_{\\text{unembed}}$ (or $z_j = \\sum_k h_k W_{kj}$).\n",
    "\n",
    "**Gradient w.r.t. h:**\n",
    "$$\\frac{\\partial L}{\\partial h_k} = \\sum_j \\frac{\\partial L}{\\partial z_j} \\frac{\\partial z_j}{\\partial h_k} = \\sum_j \\frac{\\partial L}{\\partial z_j} W_{kj}$$\n",
    "\n",
    "In matrix form: $\\frac{\\partial L}{\\partial h} = W_{\\text{unembed}} \\cdot \\frac{\\partial L}{\\partial z}$\n",
    "\n",
    "**Gradient w.r.t. W_unembed:**\n",
    "$$\\frac{\\partial L}{\\partial W_{kj}} = \\frac{\\partial L}{\\partial z_j} \\cdot h_k$$\n",
    "\n",
    "In matrix form: $\\frac{\\partial L}{\\partial W_{\\text{unembed}}} = h^T \\cdot \\frac{\\partial L}{\\partial z}$ (outer product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "backward-linear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 backward: ∂L/∂h and ∂L/∂W_unembed\n",
      "\n",
      "  ∂L/∂h = W_unembed @ ∂L/∂z\n",
      "  ∂L/∂h = ['-0.1932', '-0.0841', '-0.0841']\n",
      "\n",
      "  ∂L/∂W_unembed = outer(h, ∂L/∂z)\n",
      "  Shape: torch.Size([3, 4])\n",
      "  ∂L/∂W_unembed = \n",
      "[[ 0.      0.      0.     -0.    ]\n",
      " [ 0.0841  0.1386  0.1386 -0.3614]\n",
      " [ 0.0841  0.1386  0.1386 -0.3614]]\n"
     ]
    }
   ],
   "source": [
    "# Step 3 backward: ∂L/∂h and ∂L/∂W_unembed\n",
    "\n",
    "# Gradient w.r.t. h\n",
    "dL_dh = W_unembed.detach() @ dL_dz  # [d_model, vocab] @ [vocab] = [d_model]\n",
    "\n",
    "# Gradient w.r.t. W_unembed (outer product)\n",
    "dL_dW_unembed = torch.outer(h.detach(), dL_dz)  # [d_model] outer [vocab] = [d_model, vocab]\n",
    "\n",
    "print(\"Step 3 backward: ∂L/∂h and ∂L/∂W_unembed\")\n",
    "print(f\"\\n  ∂L/∂h = W_unembed @ ∂L/∂z\")\n",
    "print(f\"  ∂L/∂h = {[f'{x:.4f}' for x in dL_dh.tolist()]}\")\n",
    "print(f\"\\n  ∂L/∂W_unembed = outer(h, ∂L/∂z)\")\n",
    "print(f\"  Shape: {dL_dW_unembed.shape}\")\n",
    "print(f\"  ∂L/∂W_unembed = \")\n",
    "print(dL_dW_unembed.numpy().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backward-avg-intro",
   "metadata": {},
   "source": [
    "### Step 2 (backward): ∂L/∂e (through averaging)\n",
    "\n",
    "We had $h = \\frac{1}{n} \\sum_i e_i$.\n",
    "\n",
    "Each embedding contributes equally, so:\n",
    "$$\\frac{\\partial L}{\\partial e_i} = \\frac{1}{n} \\frac{\\partial L}{\\partial h}$$\n",
    "\n",
    "The gradient is just copied to each position, scaled by 1/n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "backward-avg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 backward: ∂L/∂e\n",
      "  ∂L/∂e = (1/2) × ∂L/∂h broadcast to each position\n",
      "  Shape: torch.Size([2, 3])\n",
      "  ∂L/∂e[0] = ['-0.0966', '-0.0420', '-0.0420']\n",
      "  ∂L/∂e[1] = ['-0.0966', '-0.0420', '-0.0420']\n",
      "  (Both rows are the same — the gradient is distributed equally)\n"
     ]
    }
   ],
   "source": [
    "# Step 2 backward: ∂L/∂e\n",
    "\n",
    "n = seq_len\n",
    "dL_de = dL_dh.unsqueeze(0).expand(n, -1) / n  # Copy to each position, scale by 1/n\n",
    "\n",
    "print(\"Step 2 backward: ∂L/∂e\")\n",
    "print(f\"  ∂L/∂e = (1/{n}) × ∂L/∂h broadcast to each position\")\n",
    "print(f\"  Shape: {dL_de.shape}\")\n",
    "print(f\"  ∂L/∂e[0] = {[f'{x:.4f}' for x in dL_de[0].tolist()]}\")\n",
    "print(f\"  ∂L/∂e[1] = {[f'{x:.4f}' for x in dL_de[1].tolist()]}\")\n",
    "print(f\"  (Both rows are the same — the gradient is distributed equally)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backward-embed-intro",
   "metadata": {},
   "source": [
    "### Step 1 (backward): ∂L/∂W_embed (through embedding lookup)\n",
    "\n",
    "The embedding lookup selected rows from W_embed. The gradient only affects those rows:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_{\\text{embed}}[i]} = \\begin{cases} \\frac{\\partial L}{\\partial e_j} & \\text{if token } j \\text{ = } i \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "Only the rows we actually used get gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "backward-embed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 backward: ∂L/∂W_embed\n",
      "  Input tokens: [1, 2]\n",
      "  Only rows [1, 2] get gradients\n",
      "\n",
      "  ∂L/∂W_embed = \n",
      "[[ 0.      0.      0.    ]\n",
      " [-0.0966 -0.042  -0.042 ]\n",
      " [-0.0966 -0.042  -0.042 ]\n",
      " [ 0.      0.      0.    ]]\n",
      "\n",
      "  Row 0 (token 0): [0.0, 0.0, 0.0] (not used)\n",
      "  Row 1 (token 1): ['-0.0966', '-0.0420', '-0.0420'] (used once)\n",
      "  Row 2 (token 2): ['-0.0966', '-0.0420', '-0.0420'] (used once)\n",
      "  Row 3 (token 3): [0.0, 0.0, 0.0] (not used)\n"
     ]
    }
   ],
   "source": [
    "# Step 1 backward: ∂L/∂W_embed\n",
    "\n",
    "dL_dW_embed = torch.zeros_like(W_embed)\n",
    "\n",
    "# Scatter the gradients to the rows that were used\n",
    "for i, token_id in enumerate(input_tokens):\n",
    "    dL_dW_embed[token_id] += dL_de[i]\n",
    "\n",
    "print(\"Step 1 backward: ∂L/∂W_embed\")\n",
    "print(f\"  Input tokens: {input_tokens.tolist()}\")\n",
    "print(f\"  Only rows {input_tokens.tolist()} get gradients\")\n",
    "print(f\"\")\n",
    "print(f\"  ∂L/∂W_embed = \")\n",
    "print(dL_dW_embed.numpy().round(4))\n",
    "print(f\"\")\n",
    "print(f\"  Row 0 (token 0): {dL_dW_embed[0].tolist()} (not used)\")\n",
    "print(f\"  Row 1 (token 1): {[f'{x:.4f}' for x in dL_dW_embed[1].tolist()]} (used once)\")\n",
    "print(f\"  Row 2 (token 2): {[f'{x:.4f}' for x in dL_dW_embed[2].tolist()]} (used once)\")\n",
    "print(f\"  Row 3 (token 3): {dL_dW_embed[3].tolist()} (not used)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Verification: Compare with PyTorch Autograd\n",
    "\n",
    "Now let's verify our manual gradients match what PyTorch computes with `.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "verify-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (verification): 1.2827\n",
      "Loss (original):     1.2827\n"
     ]
    }
   ],
   "source": [
    "# Re-run forward pass with fresh tensors (gradients attached)\n",
    "W_embed_v = torch.tensor([\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [1.0, 1.0, 1.0],\n",
    "], requires_grad=True)\n",
    "\n",
    "W_unembed_v = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.5],\n",
    "    [0.0, 1.0, 0.0, 0.5],\n",
    "    [0.0, 0.0, 1.0, 0.5],\n",
    "], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "e_v = W_embed_v[input_tokens]\n",
    "h_v = e_v.mean(dim=0)\n",
    "z_v = h_v @ W_unembed_v\n",
    "p_v = F.softmax(z_v, dim=0)\n",
    "L_v = -torch.log(p_v[target])\n",
    "\n",
    "print(f\"Loss (verification): {L_v.item():.4f}\")\n",
    "print(f\"Loss (original):     {L.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "verify-backward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch autograd gradients:\n",
      "\n",
      "∂L/∂W_embed (PyTorch):\n",
      "[[ 0.      0.      0.    ]\n",
      " [-0.0966 -0.042  -0.042 ]\n",
      " [-0.0966 -0.042  -0.042 ]\n",
      " [ 0.      0.      0.    ]]\n",
      "\n",
      "∂L/∂W_embed (manual):\n",
      "[[ 0.      0.      0.    ]\n",
      " [-0.0966 -0.042  -0.042 ]\n",
      " [-0.0966 -0.042  -0.042 ]\n",
      " [ 0.      0.      0.    ]]\n",
      "\n",
      "∂L/∂W_unembed (PyTorch):\n",
      "[[ 0.      0.      0.      0.    ]\n",
      " [ 0.0841  0.1386  0.1386 -0.3614]\n",
      " [ 0.0841  0.1386  0.1386 -0.3614]]\n",
      "\n",
      "∂L/∂W_unembed (manual):\n",
      "[[ 0.      0.      0.     -0.    ]\n",
      " [ 0.0841  0.1386  0.1386 -0.3614]\n",
      " [ 0.0841  0.1386  0.1386 -0.3614]]\n"
     ]
    }
   ],
   "source": [
    "# Run backward pass\n",
    "L_v.backward()\n",
    "\n",
    "print(\"PyTorch autograd gradients:\")\n",
    "print(f\"\\n∂L/∂W_embed (PyTorch):\")\n",
    "print(W_embed_v.grad.numpy().round(4))\n",
    "print(f\"\\n∂L/∂W_embed (manual):\")\n",
    "print(dL_dW_embed.numpy().round(4))\n",
    "\n",
    "print(f\"\\n∂L/∂W_unembed (PyTorch):\")\n",
    "print(W_unembed_v.grad.numpy().round(4))\n",
    "print(f\"\\n∂L/∂W_unembed (manual):\")\n",
    "print(dL_dW_unembed.numpy().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "verify-match",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "VERIFICATION\n",
      "==================================================\n",
      "W_embed gradients match:   True ✓\n",
      "W_unembed gradients match: True ✓\n"
     ]
    }
   ],
   "source": [
    "# Check they match\n",
    "embed_match = torch.allclose(W_embed_v.grad, dL_dW_embed, atol=1e-5)\n",
    "unembed_match = torch.allclose(W_unembed_v.grad, dL_dW_unembed, atol=1e-5)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"W_embed gradients match:   {embed_match} ✓\" if embed_match else f\"W_embed gradients match:   {embed_match} ✗\")\n",
    "print(f\"W_unembed gradients match: {unembed_match} ✓\" if unembed_match else f\"W_unembed gradients match: {unembed_match} ✗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Visualizing the Flow\n",
    "\n",
    "Let's see the complete picture: forward values and backward gradients side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "visualize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "THE COMPLETE PICTURE\n",
      "============================================================\n",
      "\n",
      "FORWARD PASS (left to right)\n",
      "------------------------------------------------------------\n",
      "tokens: [1, 2]\n",
      "   ↓ embedding lookup\n",
      "e: [[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]\n",
      "   ↓ average\n",
      "h: [0.0, 0.5, 0.5]\n",
      "   ↓ linear (× W_unembed)\n",
      "z: ['0.000', '0.500', '0.500', '0.500']\n",
      "   ↓ softmax\n",
      "p: ['0.168', '0.277', '0.277', '0.277']\n",
      "   ↓ -log(p[target=3])\n",
      "L: 1.2827\n",
      "\n",
      "BACKWARD PASS (right to left)\n",
      "------------------------------------------------------------\n",
      "∂L/∂L: 1 (by definition)\n",
      "   ↓\n",
      "∂L/∂p: ['0.000', '0.000', '0.000', '-3.607']\n",
      "   ↓ softmax + cross-entropy (combined)\n",
      "∂L/∂z: ['0.168', '0.277', '0.277', '-0.723']  (= p - y_onehot)\n",
      "   ↓ linear\n",
      "∂L/∂h: ['-0.193', '-0.084', '-0.084']\n",
      "   ↓ average (divide by n=2)\n",
      "∂L/∂e: [['-0.097', '-0.042', '-0.042'], ['-0.097', '-0.042', '-0.042']]\n",
      "   ↓ scatter to used rows\n",
      "∂L/∂W_embed: (see matrix above)\n",
      "\n",
      "WEIGHT GRADIENTS (what optimizer.step() uses)\n",
      "------------------------------------------------------------\n",
      "∂L/∂W_embed shape:   torch.Size([4, 3])\n",
      "∂L/∂W_unembed shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"THE COMPLETE PICTURE\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"FORWARD PASS (left to right)\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"tokens: {input_tokens.tolist()}\")\n",
    "print(f\"   ↓ embedding lookup\")\n",
    "print(f\"e: [{e[0].detach().tolist()}, {e[1].detach().tolist()}]\")\n",
    "print(f\"   ↓ average\")\n",
    "print(f\"h: {h.detach().tolist()}\")\n",
    "print(f\"   ↓ linear (× W_unembed)\")\n",
    "print(f\"z: {[f'{x:.3f}' for x in z.detach().tolist()]}\")\n",
    "print(f\"   ↓ softmax\")\n",
    "print(f\"p: {[f'{x:.3f}' for x in p.detach().tolist()]}\")\n",
    "print(f\"   ↓ -log(p[target={target.item()}])\")\n",
    "print(f\"L: {L.item():.4f}\")\n",
    "print()\n",
    "print(\"BACKWARD PASS (right to left)\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"∂L/∂L: 1 (by definition)\")\n",
    "print(f\"   ↓\")\n",
    "print(f\"∂L/∂p: {[f'{x:.3f}' for x in dL_dp.tolist()]}\")\n",
    "print(f\"   ↓ softmax + cross-entropy (combined)\")\n",
    "print(f\"∂L/∂z: {[f'{x:.3f}' for x in dL_dz.tolist()]}  (= p - y_onehot)\")\n",
    "print(f\"   ↓ linear\")\n",
    "print(f\"∂L/∂h: {[f'{x:.3f}' for x in dL_dh.tolist()]}\")\n",
    "print(f\"   ↓ average (divide by n={seq_len})\")\n",
    "print(f\"∂L/∂e: [{[f'{x:.3f}' for x in dL_de[0].tolist()]}, {[f'{x:.3f}' for x in dL_de[1].tolist()]}]\")\n",
    "print(f\"   ↓ scatter to used rows\")\n",
    "print(f\"∂L/∂W_embed: (see matrix above)\")\n",
    "print()\n",
    "print(\"WEIGHT GRADIENTS (what optimizer.step() uses)\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"∂L/∂W_embed shape:   {dL_dW_embed.shape}\")\n",
    "print(f\"∂L/∂W_unembed shape: {dL_dW_unembed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: What Backpropagation Actually Does\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "1. **Forward pass:** Compute all intermediate values, building a computation graph.\n",
    "\n",
    "2. **Backward pass:** Starting from the loss, propagate gradients backward:\n",
    "   - At each node, receive ∂L/∂output from above\n",
    "   - Compute ∂L/∂input (to pass backward) and ∂L/∂weights (to update parameters)\n",
    "   - The chain rule connects them: ∂L/∂input = ∂L/∂output × ∂output/∂input\n",
    "\n",
    "3. **Update:** Use the weight gradients to adjust parameters toward lower loss.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Gradients flow backward** through the same path values flowed forward\n",
    "- **The chain rule** multiplies gradients at each step\n",
    "- **Only used weights get gradients** (sparse updates for embedding lookups)\n",
    "- **Cross-entropy + softmax simplifies beautifully:** ∂L/∂z = p - y\n",
    "\n",
    "### What PyTorch Does Automatically\n",
    "\n",
    "Every operation in PyTorch records:\n",
    "- What inputs it received\n",
    "- How to compute ∂output/∂input\n",
    "\n",
    "When you call `.backward()`, it walks this graph in reverse, applying the chain rule at each step. That's it. That's the magic.\n",
    "\n",
    "---\n",
    "\n",
    "*Now when you see `loss.backward()`, you know what's happening inside.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
