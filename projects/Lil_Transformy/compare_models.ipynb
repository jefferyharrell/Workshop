{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Model Comparison: The Evolution\n",
    "\n",
    "*From word salad to... hopefully something better.*\n",
    "\n",
    "---\n",
    "\n",
    "This notebook loads all trained models from the Lil Transformy sequence and compares their outputs on the same prompts. Run it whenever you want to see how far we've come.\n",
    "\n",
    "**Models in the main sequence:**\n",
    "- 03: Bigram (embed → unembed)\n",
    "- 04: + Attention\n",
    "- 05: + Positional encoding\n",
    "- 06: + FFN\n",
    "- 07: + Residuals & LayerNorm ← THE FISH HAS A SPINE\n",
    "- 08: + Stacked blocks (coming soon)\n",
    "- 09: + Multi-head attention (coming soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Reproducibility for generation\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tokenizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4,096\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "class LilTokenizer:\n",
    "    \"\"\"Compact tokenizer for Lil Transformy.\"\"\"\n",
    "    \n",
    "    def __init__(self, gpt2_to_compact, compact_to_gpt2, vocab_size):\n",
    "        self.gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "        self.gpt2_to_compact = gpt2_to_compact\n",
    "        self.compact_to_gpt2 = compact_to_gpt2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_id = 0\n",
    "        self.unk_id = 1\n",
    "        self.eos_id = 2\n",
    "    \n",
    "    def encode(self, text, add_eos=True):\n",
    "        gpt2_tokens = self.gpt2_tokenizer.encode(text)\n",
    "        compact_tokens = [self.gpt2_to_compact.get(t, self.unk_id) for t in gpt2_tokens]\n",
    "        if add_eos:\n",
    "            compact_tokens.append(self.eos_id)\n",
    "        return compact_tokens\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        gpt2_tokens = []\n",
    "        for tid in token_ids:\n",
    "            if tid in [self.pad_id, self.unk_id, self.eos_id]:\n",
    "                continue\n",
    "            if tid in self.compact_to_gpt2:\n",
    "                gpt2_tokens.append(self.compact_to_gpt2[tid])\n",
    "        return self.gpt2_tokenizer.decode(gpt2_tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        with open(path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        gpt2_to_compact = {int(k): v for k, v in config['gpt2_to_compact'].items()}\n",
    "        compact_to_gpt2 = {int(k): v for k, v in config['compact_to_gpt2'].items()}\n",
    "        return cls(gpt2_to_compact, compact_to_gpt2, config['vocab_size'])\n",
    "\n",
    "\n",
    "tokenizer = LilTokenizer.load('tokenizer/tokenizer.json')\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Definitions\n",
    "\n",
    "We need to define each model architecture to load the weights. This will grow as we add more notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "model-03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 03: Bigram ===\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    \"\"\"Notebook 03: Simplest autoregressive model. Each position predicts next from itself only.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.unembed(self.embedding(x))\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Bigram only looks at last token\n",
    "                x = torch.tensor([[tokens[-1]]], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, 0] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:  # EOS\n",
    "                    break\n",
    "        \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "model-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 04: Attention ===\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Single-head causal self-attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        mask = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
    "        self.register_buffer('mask', mask)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        Q, K, V = self.W_q(x), self.W_k(x), self.W_v(x)\n",
    "        scores = (Q @ K.transpose(-2, -1)) / self.scale\n",
    "        scores = scores.masked_fill(self.mask[:T, :T], float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        return self.W_o(attn @ V)\n",
    "\n",
    "\n",
    "class AttentionLM(nn.Module):\n",
    "    \"\"\"Notebook 04: Bigram + single-head attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.attention = CausalSelfAttention(d_model, max_seq_len)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.unembed(self.attention(self.embedding(x)))\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                context = tokens[-256:]\n",
    "                x = torch.tensor([context], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "# === 05: Attention + Position ===\n",
    "\n",
    "class PositionalAttentionLM(nn.Module):\n",
    "    \"\"\"Notebook 05: Attention + learned positional embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.attention = CausalSelfAttention(d_model, max_seq_len)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))\n",
    "        return self.unembed(self.attention(tok_emb + pos_emb))\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                context = tokens[-self.max_seq_len:]\n",
    "                x = torch.tensor([context], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "# === 06: Attention + Position + FFN ===\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feedforward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff=None):\n",
    "        super().__init__()\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "\n",
    "class AttentionFFNLM(nn.Module):\n",
    "    \"\"\"Notebook 06: Attention + Position + FFN.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, d_ff=None, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.attention = CausalSelfAttention(d_model, max_seq_len)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))\n",
    "        attended = self.attention(tok_emb + pos_emb)\n",
    "        processed = self.ffn(attended)\n",
    "        return self.unembed(processed)\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                context = tokens[-self.max_seq_len:]\n",
    "                x = torch.tensor([context], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "# === 07: Transformer Block (Residuals + LayerNorm) ===\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single transformer block with pre-norm architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff=None, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.attention = CausalSelfAttention(d_model, max_seq_len)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "    \"\"\"Notebook 07: Proper transformer block with residuals and LayerNorm.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, d_ff=None, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.block = TransformerBlock(d_model, d_ff, max_seq_len)\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.block(x)\n",
    "        x = self.ln_final(x)\n",
    "        return self.unembed(x)\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                context = tokens[-self.max_seq_len:]\n",
    "                x = torch.tensor([context], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "model-placeholders",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architectures defined.\n"
     ]
    }
   ],
   "source": [
    "# === Future models will be added here ===\n",
    "#\n",
    "# 08: StackedTransformerLM\n",
    "# 09: MultiHeadTransformerLM\n",
    "#\n",
    "# Each will be added as we build the notebooks.\n",
    "\n",
    "print(\"Model architectures defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load Available Models\n",
    "\n",
    "Check which checkpoints exist and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "load-models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "============================================================\n",
      "✓ 03: Bigram\n",
      "    Parameters: 1,052,672\n",
      "    Final perplexity: 35.8\n",
      "✓ 04: + Attention (3 ep)\n",
      "    Parameters: 1,118,208\n",
      "    Final perplexity: 25.0\n",
      "✓ 05: + Position (1 ep)\n",
      "    Parameters: 1,150,976\n",
      "    Final perplexity: 17.7\n",
      "✓ 06: + FFN (1 ep)\n",
      "    Parameters: 1,282,688\n",
      "    Final perplexity: 13.4\n",
      "✓ 07: + Residual (1 ep)\n",
      "    Parameters: 1,283,456\n",
      "    Final perplexity: 10.9\n",
      "\n",
      "Loaded 5 models.\n"
     ]
    }
   ],
   "source": [
    "# Registry of models: (checkpoint_file, model_class, display_name, extra_kwargs)\n",
    "MODEL_REGISTRY = [\n",
    "    ('03_bigram.pt', BigramLM, '03: Bigram', {}),\n",
    "    ('04_attention.pt', AttentionLM, '04: + Attention (3 ep)', {}),\n",
    "    ('05_positional.pt', PositionalAttentionLM, '05: + Position (1 ep)', {'max_seq_len': 256}),\n",
    "    ('06_ffn.pt', AttentionFFNLM, '06: + FFN (1 ep)', {'max_seq_len': 256}),\n",
    "    ('07_transformer_block.pt', TransformerLM, '07: + Residual (1 ep)', {'max_seq_len': 256}),\n",
    "    # Future:\n",
    "    # ('08_stacked.pt', ..., '08: Stacked'),\n",
    "    # ('09_multihead.pt', ..., '09: Multi-Head'),\n",
    "]\n",
    "\n",
    "models = {}\n",
    "stats = {}\n",
    "\n",
    "print(\"Loading models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for checkpoint_file, model_class, name, extra_kwargs in MODEL_REGISTRY:\n",
    "    path = Path(checkpoint_file)\n",
    "    if path.exists():\n",
    "        checkpoint = torch.load(path, map_location='cpu', weights_only=False)\n",
    "        \n",
    "        # Handle d_ff for FFN models\n",
    "        kwargs = {'vocab_size': checkpoint['vocab_size'], 'd_model': checkpoint['d_model']}\n",
    "        kwargs.update(extra_kwargs)\n",
    "        if 'd_ff' in checkpoint:\n",
    "            kwargs['d_ff'] = checkpoint['d_ff']\n",
    "        \n",
    "        # Create model\n",
    "        model = model_class(**kwargs)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        models[name] = model\n",
    "        stats[name] = {\n",
    "            'params': sum(p.numel() for p in model.parameters()),\n",
    "            'final_ppl': checkpoint['history']['val_perplexity'][-1]\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ {name}\")\n",
    "        print(f\"    Parameters: {stats[name]['params']:,}\")\n",
    "        print(f\"    Final perplexity: {stats[name]['final_ppl']:.1f}\")\n",
    "    else:\n",
    "        print(f\"✗ {name} (not found: {checkpoint_file})\")\n",
    "\n",
    "print()\n",
    "print(f\"Loaded {len(models)} models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Side-by-Side Generation\n",
    "\n",
    "Give all models the same prompt, see what they produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "compare-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generations(prompt, max_tokens=50, temperature=1.0, seed=None):\n",
    "    \"\"\"\n",
    "    Generate from all loaded models with the same prompt.\n",
    "\n",
    "    If seed is provided, it's combined with a hash of the prompt so that:\n",
    "    - Same prompt + same seed = reproducible across runs\n",
    "    - Different prompts + same seed = different outputs (not stuck in attractors)\n",
    "    - All models for a given prompt get the same seed (fair comparison)\n",
    "    \"\"\"\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_eos=False)\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Combine seed with prompt hash so different prompts get different randomness\n",
    "    if seed is not None:\n",
    "        prompt_seed = seed + hash(prompt) % 10000\n",
    "    else:\n",
    "        prompt_seed = None\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if prompt_seed is not None:\n",
    "            torch.manual_seed(prompt_seed)\n",
    "\n",
    "        generated = model.generate(prompt_tokens, max_new_tokens=max_tokens, temperature=temperature)\n",
    "        text = tokenizer.decode(generated)\n",
    "\n",
    "        ppl = stats[name]['final_ppl']\n",
    "        print(f\"\\n{name} (ppl={ppl:.1f}):\")\n",
    "        print(f\"  {text}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "classic-prompts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# CLASSIC PROMPTS\n",
      "######################################################################\n",
      "======================================================================\n",
      "PROMPT: Once upon a time\n",
      "======================================================================\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "  , \"It's go away. The sun. They walked. The next to row of toys we can play with a cat's phone. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"Here, let you to touch it.\n",
      "\n",
      "But she noticed that he\n",
      "\n",
      "04: + Attention (3 ep) (ppl=25.0):\n",
      "  , there was a time-. Every morning, the comet noticed a real starfish in a little bug. There, the hole in the park and jump high. The tree?\"\n",
      "\n",
      "The grass and it felt very special village. She remembered\n",
      "\n",
      "05: + Position (1 ep) (ppl=17.7):\n",
      "  , there was a dog named Buddy. Buddy Buddy Buddy loved. Buddy Buddy Buddy Buddy loved eat food. Buddy loved horses and eat yummy.\n",
      " One day, he had found lots of wild to touch it.\n",
      "\n",
      "As she noticed that he\n",
      "\n",
      "06: + FFN (1 ep) (ppl=13.4):\n",
      "  , there were two friends named Lily. One day, they decided to explore the forest to eat food.\n",
      "\n",
      "\"Let's go and play together!\" Lily said.\n",
      "\n",
      "As they traveled to a mill. Lily was excited she noticed that the\n",
      "\n",
      "07: + Residual (1 ep) (ppl=10.9):\n",
      "  , there were two friends, Jenny and Mary. They walked together until a big row scattered toys everywhere.\n",
      "\n",
      "One of the dependable clown was walking through the woods when she saw a wild rabbit. \"Be done, Harry! I saw a\n",
      "\n",
      "======================================================================\n",
      "PROMPT: The little girl\n",
      "======================================================================\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "   called Suzy was so she was happy andbles. The seal started playing in the fire.\"\n",
      "\n",
      "The bull help him. They start. She had to make fun. Please!\"\n",
      "Amy was my dog. You look around him a fence\n",
      "\n",
      "04: + Attention (3 ep) (ppl=25.0):\n",
      "   She was happy twins to play together and always wanted to start!\n",
      "\n",
      "05: + Position (1 ep) (ppl=17.7):\n",
      "   was very happy to help. The little boat is really nice!\n",
      "\n",
      "The little girl didn't move again. He felt very sad and he knew to be to make angry. She said, \"Max my dog pet!\" The girl said, \"\n",
      "\n",
      "06: + FFN (1 ep) (ppl=13.4):\n",
      "   flew away from Lily and she was happy that she could start!\n",
      "\n",
      "07: + Residual (1 ep) (ppl=10.9):\n",
      "   was very happy, and she let her do yoga really well!\n",
      "\n",
      "======================================================================\n",
      "PROMPT: He was very\n",
      "======================================================================\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "   impressed.\n",
      "One day full sheet near a ticket was to get it was a great job. One day, they whispered, they had been exploring the room and the children gathered around the tree.\n",
      "Mark needed to make a little girl named Lily\n",
      "\n",
      "04: + Attention (3 ep) (ppl=25.0):\n",
      "   impressed. He decided to make sheet it up into the sheet to try it soar high in the sheet and smiled and learned a lesson. He pretended they did not listen to his mom or before he pushed the sheet and that it work hard lesson. One\n",
      "\n",
      "05: + Position (1 ep) (ppl=17.7):\n",
      "   mad at him. He ran to up into the sea to shore. He had gave him a big and said, \"Thank you, strong! And very beautiful, that day, Bob and the crowd winked and said, \" good you nodded\n",
      "\n",
      "06: + FFN (1 ep) (ppl=13.4):\n",
      "   mad at him.\n",
      "\n",
      "\", my twins,\" Sam said. \"What did she like a toy!\"\n",
      "\n",
      "His mommy smiled and said, \"It is a joke?\" \" the man winked that fairy like much good things.\n",
      "\n",
      "07: + Residual (1 ep) (ppl=10.9):\n",
      "   mad.\n",
      "\n",
      "At this sheet was a bit uncomfortable, but he was too fast they could like him and he learned a lesson. He always promised from other people who knows how to share and tell them.\n",
      "\n",
      "He also had good things that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The classics\n",
    "print(\"\\n\" + \"#\" * 70)\n",
    "print(\"# CLASSIC PROMPTS\")\n",
    "print(\"#\" * 70)\n",
    "\n",
    "compare_generations(\"Once upon a time\", seed=42)\n",
    "compare_generations(\"The little girl\", seed=42)\n",
    "compare_generations(\"He was very\", seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "story-prompts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# STORY STARTERS\n",
      "######################################################################\n",
      "======================================================================\n",
      "PROMPT: Once upon a time there was a little girl named Lily. She\n",
      "======================================================================\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "   was very angry.\n",
      "Sally agreed.\n",
      "\n",
      "\"Mom put the baby found a jungle with your cars again. They see a.\n",
      "The bird is shiny and ran to find it flew away. He went when he got ready to the\n",
      "\n",
      "04: + Attention (3 ep) (ppl=25.0):\n",
      "   loved very creative girl. One day Sarah asked her dad, \"Why you are sad and told her cat to visit her not afraid to go home now they all the dream?\" she asked her, so so she asked her when it got to bake the\n",
      "\n",
      "05: + Position (1 ep) (ppl=17.7):\n",
      "   loved to write. She loved her. One day, the world was a baby when she went with a towel everywhere she went for a.\n",
      " away next morning?\" she asked her mom.\n",
      "\n",
      "\"Mom said, \"We're sorry\n",
      "\n",
      "06: + FFN (1 ep) (ppl=13.4):\n",
      "   loved to play outside. One day, she saw a huge mountain near her house. She was so excited!\n",
      "\n",
      "\"What a?\" Lily asked. \"My friends ran to find it?\"\n",
      "\n",
      "\"What is that mist mean?\" the\n",
      "\n",
      "07: + Residual (1 ep) (ppl=10.9):\n",
      "   loved to write and write. One day, she went to the park with her mom.\n",
      "\n",
      "As they took a big bite, Lily noticed a vase next shiny and ran to her mom.\n",
      "\n",
      "\"Mommy, my little one can\n",
      "\n",
      "======================================================================\n",
      "PROMPT: The big dog and the small cat were\n",
      "======================================================================\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "   back to play a big duck swimming, coming up and Ben say, car.\n",
      "One day, there was a jar was getting dirty part of them anymore.\n",
      "Lily, and smiled and said, the tree. and fast as she was\n",
      "\n",
      "04: + Attention (3 ep) (ppl=25.0):\n",
      "   back to play again. The dog went off, and Ben pushed the car the letter. The cat had a mechanic heard a man saw the ball would stay put the vet stopped. The owner was friendly dog caught the band again and fast. The road\n",
      "\n",
      "05: + Position (1 ep) (ppl=17.7):\n",
      "   back to play again. The dog went off and helped the dog his car the dog. The dog had big bone away. Tim saw the ball and the sound of the dog. The dog was friendly dog named wagged his tail.\n",
      "\n",
      "Tom\n",
      "\n",
      "06: + FFN (1 ep) (ppl=13.4):\n",
      "   back in the hand. The dog went too big and kept rocking his car away safely.\n",
      "\n",
      "The cat mechanic heard his toy dog blow. It was dead and he stopped. The owner was friendly and the toy band took the toy box on the\n",
      "\n",
      "07: + Residual (1 ep) (ppl=10.9):\n",
      "   back in sight.\n",
      "\n",
      "The dog: The dog was very happy. It smelled amazing and the smoke sound! The dog saw the ball and was put ice cubes on the ceiling. The smoke turned the ceiling on and fast. The dog\n",
      "\n",
      "======================================================================\n",
      "PROMPT: One sunny day, the children went to the park to\n",
      "======================================================================\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "   the birds. He felt like a moment. All the lady.\" The gym. She looked up.\n",
      "Sue ate the park. Now. You must be extra special surprise for her and started to help him. He was boss of her friend.\n",
      "\n",
      "04: + Attention (3 ep) (ppl=25.0):\n",
      "   the birds and birds and saw a squirrel led her to go outside. The old man standing up. The little bear ate too. The lady. The other kids play with all the grumpy started to help him to him to the boy said, \"\n",
      "\n",
      "05: + Position (1 ep) (ppl=17.7):\n",
      "   pretend he saw birds and the birds. She wanted to go outside. So, they decided to look see where they try. The ball one. But be careful better. When he got to her. Finally, they was the storm went to the\n",
      "\n",
      "06: + FFN (1 ep) (ppl=13.4):\n",
      "   play. They saw a big, green field, and some rain. The old man had a boat. The boat ate until it was a secret and he could play with all the other animals. And they lived happily ever after.\n",
      "\n",
      "07: + Residual (1 ep) (ppl=10.9):\n",
      "   play. One day, he saw a boy crying on a slide. The mean boy wanted to help the little boy. He didn't want to go to the park.\n",
      "\n",
      "So the boy gathered the huge wheel and asked the boy to help.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Story starters\n",
    "print(\"\\n\" + \"#\" * 70)\n",
    "print(\"# STORY STARTERS\")\n",
    "print(\"#\" * 70)\n",
    "\n",
    "compare_generations(\"Once upon a time there was a little girl named Lily. She\", seed=42)\n",
    "compare_generations(\"The big dog and the small cat were\", seed=42)\n",
    "compare_generations(\"One sunny day, the children went to the park to\", seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "challenging-prompts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# CONTEXT-DEPENDENT PROMPTS\n",
      "######################################################################\n",
      "======================================================================\n",
      "PROMPT: Lily had a red ball. She loved to play with her\n",
      "======================================================================\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "   imagination and ran back to throw things. They liked to move really fast and started to a notebook was sad because it. She knew it.\"\n",
      "Joe said. She was a big red feather right. She stirred.\n",
      "And that?\"\n",
      "\n",
      "\n",
      "\n",
      "04: + Attention (3 ep) (ppl=25.0):\n",
      "   imagination and ran back to throw things. They ran to their heads. Their cries too.\n",
      "Lily and hugged her shoe and showed them her brother, but she was very much. She always are safe.\n",
      "\n",
      "\n",
      "\n",
      "One day,\n",
      "\n",
      "05: + Position (1 ep) (ppl=17.7):\n",
      "   friends and ran back to the ball until he finally got tired really.\n",
      "\n",
      "06: + FFN (1 ep) (ppl=13.4):\n",
      "   toys and ran back to their toys. They liked to play with her toys and books.\n",
      "\n",
      "One day, they saw a big dog playing with the things. They wanted some people into the. They named Tom, was mean in the\n",
      "\n",
      "07: + Residual (1 ep) (ppl=10.9):\n",
      "   doll and ran after it all day. They would push and fight. They could not find anything, but they did not make a cookie or a fight anymore. They missed each other.\n",
      "\n",
      "======================================================================\n",
      "PROMPT: The boy was sad because his toy was broken. His mom said\n",
      "======================================================================\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "   her mom asked her mouth. He didn't be the ball flies who asked Jane was so much fun toys in the yard with those who wanted to be selfish bird asked his wife, asking. Sure enough to promise. the park, do it was\n",
      "\n",
      "04: + Attention (3 ep) (ppl=25.0):\n",
      "   yes and asked him. The dad said \" mister kids said, goodbye to be nice.\"\n",
      "The boy didn't know what to it's be selfish. He wanted to be kind and play. But promise. He didn't want to see what\n",
      "\n",
      "05: + Position (1 ep) (ppl=17.7):\n",
      "   her mommy no. The dad didn't want the baby go inside!\" Her mom answered and they could go back yard with those who can help be a very brave and sound. She likes to be happy.\n",
      "\n",
      "Lila and Ben see their\n",
      "\n",
      "06: + FFN (1 ep) (ppl=13.4):\n",
      "  , \"Be careful. I want to get up the back feeling silly!\" He was so relieved and free.\n",
      "\n",
      "The little insect joined him and continued his brave blue sound, asking and off he never promise can help the insect anymore. The moral\n",
      "\n",
      "07: + Residual (1 ep) (ppl=10.9):\n",
      "   her and asked what happened.\n",
      "\n",
      "Lily was hurt. She asked Tommy, \"What happened?\"\n",
      "\n",
      "\"It's a good structure. It's raining well, just for you to be a promise. the soldier was excited and he\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# More challenging - requires context\n",
    "print(\"\\n\" + \"#\" * 70)\n",
    "print(\"# CONTEXT-DEPENDENT PROMPTS\")\n",
    "print(\"#\" * 70)\n",
    "\n",
    "# These require remembering earlier context\n",
    "compare_generations(\"Lily had a red ball. She loved to play with her\", seed=42)\n",
    "compare_generations(\"The boy was sad because his toy was broken. His mom said\", seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "stats-table",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL COMPARISON\n",
      "============================================================\n",
      "\n",
      "Model                     Parameters      Perplexity  \n",
      "----------------------------------------------------\n",
      "03: Bigram                   1,052,672       35.8\n",
      "04: + Attention (3 ep)       1,118,208       25.0\n",
      "05: + Position (1 ep)        1,150,976       17.7\n",
      "06: + FFN (1 ep)             1,282,688       13.4\n",
      "07: + Residual (1 ep)        1,283,456       10.9\n",
      "\n",
      "Lower perplexity = less surprised by correct answer = better.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"{'Model':<25} {'Parameters':<15} {'Perplexity':<12}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "for name in models.keys():\n",
    "    params = stats[name]['params']\n",
    "    ppl = stats[name]['final_ppl']\n",
    "    print(f\"{name:<25} {params:>12,}   {ppl:>8.1f}\")\n",
    "\n",
    "print()\n",
    "print(\"Lower perplexity = less surprised by correct answer = better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try Your Own Prompts\n",
    "\n",
    "Modify the cell below to test whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "interactive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROMPT: Once upon a time there was a little girl who lived in a magical forest. She\n",
      "======================================================================\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "   wanted to break their! She met a toy car and loud noise. The fireman. All of the joy! You can do not disturbed Sally was too strong house. They shouted. His mommy. She cried.\n",
      "\n",
      "\n",
      "The bird waved her and not want my toys, Lily is\n",
      "\n",
      "04: + Attention (3 ep) (ppl=25.0):\n",
      "   was amazed.\n",
      "\n",
      "05: + Position (1 ep) (ppl=17.7):\n",
      "   loved to spin around in the forest with joy. Every morning, she even in the forest, but mom the woods today when she met a great wonderful day, the forest found a sweet thing. The bird was very tasty, a fancy bird birdcage and told her that her message's mom\n",
      "\n",
      "06: + FFN (1 ep) (ppl=13.4):\n",
      "   wanted to find her way. She met a toy they would take a new home. On the way to the biggest scale, and pebbles had wonderful day!\n",
      "\n",
      "So Sarah's mom took her to a. When Grace's mom saw that her and told her it was the perfect cup\n",
      "\n",
      "07: + Residual (1 ep) (ppl=10.9):\n",
      "   wanted to find something special. She met a monkey who was loud and he was curious than any other animals saw the king's castle on the king's side of join the. The monkey said \"My. The monkey is doing the monkey. Would you like my job, Lily?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your prompt here!\n",
    "compare_generations(\n",
    "    \"Once upon a time there was a little girl who lived in a magical forest. She\",\n",
    "    max_tokens=60,\n",
    "    temperature=1.0,\n",
    "    seed=42  # Set to None for random each time\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
