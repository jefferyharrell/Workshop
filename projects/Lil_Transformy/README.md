# Lil Transformy

*An evolutionary journey from amoeba to duck.*

---

## What This Is

A series of notebooks that build a transformer language model from scratch, starting with the simplest possible architecture and adding components one at a time. Each notebook is a **complete, working model**â€”not a fragment. You train it, see what it can do, feel its limitations, then understand why the next component helps.

By the end, you'll have built something equivalent to Duckling II, but you'll understand *why* each piece is there.

---

## The Evolution

### Part 1: The Brownie (A Lesson in Wrong Turns)

| Notebook | Architecture | What's New | What We Learned |
|----------|--------------|------------|-----------------|
| **00** | â€” | Data prep | Tokenizer + corpus analysis |
| **01** | Bag of Words | Embed â†’ Average â†’ Unembed | It talks! Word soup, but it learned word co-occurrence. |
| **02** | + Positional | + Position embeddings | **Position without attention = useless.** Averaging destroys position info. |

Notebooks 01-02 taught us something important: you can't evolve a bag-of-words model into a language model by adding ingredients. The averaging operation is a dead endâ€”position information gets blended away, and tokens can't see each other. It's like trying to turn a brownie into a cake by adding frosting.

### Part 2: The Cake (Autoregressive Foundation)

| Notebook | Architecture | What's New | What It Can (Maybe) Do |
|----------|--------------|------------|------------------------|
| **03** | Bigram | Embed â†’ Unembed (per position) | Simplest autoregressive LM. One token of memory. Vaguely English word salad. |
| **04** | + Attention | + Single attention head | **Tokens can finally see each other.** Bag of predecessors, weighted by relevance. |
| **05** | + Positional | + Position embeddings | Now attention knows *where* tokens are, not just *what* they are. |
| **06** | + FFN | + Feedforward layer | Per-token "thinking" after attention gathers context. |
| **07** | Transformer Block | + Residuals + LayerNorm | Information flows properly. Training stabilizes. |
| **08** | Stacked | + Second block | Deeper patterns. |
| **09** | Multi-Head | + Multiple attention heads | Parallel attention patterns. Full transformer. |

The pivot: instead of averaging all tokens, we predict the next token from specific positions. Now we're building toward an actual autoregressive causal language modelâ€”the architecture that GPT, Claude, and Llama all use.

**Why attention before position?** Attention without position is *permutation invariant*â€”the model can see all its predecessors but doesn't know where each one is. It can still learn content-based patterns ("attend to nouns when predicting verbs"). Adding position gives it structure-based patterns ("attend to the token 2 positions back"). Each step adds something real.

---

## Philosophy

**Each model is complete.** Notebook 01 isn't "broken" or "unfinished"â€”it's a bag-of-words model, and bag-of-words models exist. You see what that architecture can and can't do. The limitation motivates the next step.

**Same data, same training.** Every model trains on the same TinyStories corpus with the same hyperparameters. The only variable is the architecture. This makes comparisons fair and differences meaningful.

**Understanding over performance.** We're not trying to beat benchmarks. We're trying to *see* what each component contributes. If the bag-of-words model outputs garbage, that's informative. If adding attention makes it form sentences, that's the lesson.

---

## The Corpus

TinyStories: synthetic children's stories generated by GPT-3.5/4, designed to be simple and repetitive. Perfect for tiny models.

Tokenizer: GPT-2 tokenizer, truncated to the minimum vocabulary that covers 99%+ of the corpus. Analysis in notebook 00.

---

## Running the Notebooks

Each notebook is standalone. Open any one, hit Restart & Run All, watch it work.

Dependencies:
- PyTorch
- Transformers (for GPT-2 tokenizer)
- Datasets (for TinyStories)
- matplotlib, numpy, tqdm

All training runs in float32 on CPU or MPS. No exotic hardware required.

---

## Lineage

This project lives in the Workshop, alongside:
- **Duckling II:** The "production" tiny model for dead token research
- **Professor Alpha:** Teaching notebooks (including attention math)
- **Azimuth:** The frozen smoke investigation

Lil Transformy is pedagogy. Duckling II is experimentation. Same family, different purposes.

---

*"Here's the amoeba. Here's the fish. Here's the lizard. Here's the mammal. Here's the duck."*

ðŸ¦†ðŸ§¬
